<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://anshprakash.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://anshprakash.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-12T09:57:08+00:00</updated><id>https://anshprakash.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">MimicPlay on Franka Arm and its Extension</title><link href="https://anshprakash.github.io/blog/2025/mimicplay/" rel="alternate" type="text/html" title="MimicPlay on Franka Arm and its Extension"/><published>2025-09-08T00:00:00+00:00</published><updated>2025-09-08T00:00:00+00:00</updated><id>https://anshprakash.github.io/blog/2025/mimicplay</id><content type="html" xml:base="https://anshprakash.github.io/blog/2025/mimicplay/"><![CDATA[<h2 id="introduction">Introduction</h2> <hr/> <h2 id="related-works">Related Works</h2> <hr/> <h2 id="mimicplay">MimicPlay</h2> <hr/> <h2 id="franka-teleoperation-system">Franka Teleoperation system</h2> <p>We developed our own teleoperation system to collect low-level demonstration data. Using a Meta Quest VR controller, we operated the Panda arm, with the headset tracking the controller’s pose in real time. The pose differences from the controller were transformed into corresponding end-effector movements on the robot, enabling us to perform various pick-and-place tasks.</p> <p>We used a Cartesian impedance controller for safer operation and additionally calibrated gravity compensation for a different gripper. This ensures that the end-effector neither drops nor unintentionally lifts depending on the load. Instructions for calibration can be found <a href="https://github.com/nbfigueroa/franka_interactive_controllers/blob/main/doc/instructions/external_tool_compensation.md">here</a>.</p> <p>Here is the code for teleoperation: <a href="https://github.com/AnshPrakash/franka_teleop"><img src="https://img.shields.io/badge/GitHub-Franka--Teleop-blue?logo=github" alt="GitHub Repo"/></a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/teleop_demo.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/teleop_demo_front.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Here is a video of Teleoperation system in action </div> <hr/> <h2 id="data-collection-pipeline">Data Collection Pipeline</h2> <h3 id="human-play-data">Human Play data</h3> <p>We store the human play data in <strong>mp4 format</strong> with a frame rate of <strong>20 FPS</strong>. Afterwards, we apply some <strong>post-processing</strong> to convert it into the required <strong>robomimic format</strong>.</p> <ol> <li> <p><strong>Hand detection</strong><br/> We use a pretrained hand detection model<a href="https://github.com/ddshan/hand_object_detector"><img src="https://img.shields.io/badge/GitHub-handobj-blue?logo=github" alt="GitHub Repo"/></a> to locate human hands in the video frames. In total, we collected <strong>10 demonstrations</strong>. After filtering, we discarded several demos where the hands could not be reliably detected.</p> </li> <li> <p><strong>3D triangulation and dataset conversion</strong><br/> Using the <strong>calibrated stereo camera setup</strong> (two synchronized viewpoints), we triangulate the detected hand positions to obtain their <strong>3D coordinates in the world frame</strong>. These 3D hand trajectories are then converted into the <strong>robomimic dataset format</strong>.</p> </li> </ol> <p>Additionaly, we also do a <strong>Projection validation (visualization check)</strong> To verify the correctness of the calibration, we re-projected the obtained 3D points back to the image plane and visually inspected their alignment with the detected 2D hand positions. This ensured that the existed <strong>camera parameters</strong> were consistent with the real-world coordinate system. Below is the detection code used for this visualization check:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">buffer/Slow_version_Human_prompts_0</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># --- Load HDF5 ---
</span><span class="n">hdf5_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/home/xiaoqi/MimicPlay/mimicplay/datasets/playdata/Slow_version_Human_prompts/demo_0_new.hdf5</span><span class="sh">"</span>   <span class="c1"># update with your file path
</span><span class="k">with</span> <span class="n">h5py</span><span class="p">.</span><span class="nc">File</span><span class="p">(</span><span class="n">hdf5_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1"># Extract robot0 end-effector positions (605, 1, 3)
</span>    <span class="n">eef_pos</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/robot0_eef_pos</span><span class="sh">"</span><span class="p">][:]</span>  <span class="c1"># shape (605,1,3)
</span>    <span class="n">eef_pos</span> <span class="o">=</span> <span class="n">eef_pos</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># now (605, 3)
</span>
    <span class="c1"># Extract images if needed
</span>    <span class="n">agentview_img</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/agentview_image</span><span class="sh">"</span><span class="p">][:]</span> 
    <span class="n">agentview_img2</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/agentview_image_2</span><span class="sh">"</span><span class="p">][:]</span> 

<span class="c1"># --- Save raw 3D positions ---
</span><span class="n">np</span><span class="p">.</span><span class="nf">savetxt</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">robot0_eef_pos.txt</span><span class="sh">"</span><span class="p">),</span> <span class="n">eef_pos</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">"</span><span class="s">%.6f</span><span class="sh">"</span><span class="p">)</span>

<span class="n">ZEDA_LEFT_CAM</span> <span class="o">=</span> <span class="nc">CameraModel</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">=</span><span class="mf">1059.9764404296875</span><span class="p">,</span>
    <span class="n">fy</span><span class="o">=</span><span class="mf">1059.9764404296875</span><span class="p">,</span>
    <span class="n">cx</span><span class="o">=</span><span class="mf">963.07568359375</span><span class="p">,</span>
    <span class="n">cy</span><span class="o">=</span><span class="mf">522.3530883789062</span><span class="p">,</span>
    <span class="n">R_wc</span><span class="o">=</span><span class="n">R</span><span class="p">.</span><span class="nf">from_quat</span><span class="p">([</span><span class="o">-</span><span class="mf">0.404974467935380</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.808551385290863</span><span class="p">,</span> <span class="mf">0.425767747250020</span><span class="p">,</span> <span class="mf">0.031018753461827</span><span class="p">]).</span><span class="nf">as_matrix</span><span class="p">(),</span>
    <span class="n">t_wc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.903701253331141</span><span class="p">,</span> <span class="mf">0.444249176547482</span><span class="p">,</span> <span class="mf">0.598645500102408</span><span class="p">])</span>
<span class="p">)</span>

<span class="n">ZEDB_RIGHT_CAM</span> <span class="o">=</span> <span class="nc">CameraModel</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">=</span><span class="mf">1060.0899658203125</span><span class="p">,</span>
    <span class="n">fy</span><span class="o">=</span><span class="mf">1059.0899658203125</span><span class="p">,</span>
    <span class="n">cx</span><span class="o">=</span><span class="mf">958.9099731445312</span><span class="p">,</span>
    <span class="n">cy</span><span class="o">=</span><span class="mf">561.5670166015625</span><span class="p">,</span>
    <span class="n">R_wc</span><span class="o">=</span><span class="n">R</span><span class="p">.</span><span class="nf">from_quat</span><span class="p">([</span><span class="mf">0.81395177</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.40028226</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.07631803</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.41404371</span><span class="p">]).</span><span class="nf">as_matrix</span><span class="p">(),</span>
    <span class="n">t_wc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.11261126</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.52195948</span><span class="p">,</span> <span class="mf">0.55795671</span><span class="p">])</span>
<span class="p">)</span>

<span class="c1"># scale factor from 1920x1080 -&gt; 640x360
</span><span class="n">sx</span> <span class="o">=</span> <span class="mf">640.0</span> <span class="o">/</span> <span class="mf">1920.0</span>   <span class="c1"># = 1/3
</span><span class="n">sy</span> <span class="o">=</span> <span class="mf">360.0</span> <span class="o">/</span> <span class="mf">1080.0</span>   <span class="c1"># = 1/3
</span>
<span class="n">ZEDA_LEFT_CAM</span>  <span class="o">=</span> <span class="n">ZEDA_LEFT_CAM</span><span class="p">.</span><span class="nf">scaled</span><span class="p">(</span><span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>
<span class="n">ZEDB_RIGHT_CAM</span> <span class="o">=</span> <span class="n">ZEDB_RIGHT_CAM</span><span class="p">.</span><span class="nf">scaled</span><span class="p">(</span><span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>


<span class="c1"># --- Project and overlay ---
</span><span class="n">left_count</span><span class="p">,</span> <span class="n">right_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>         
<span class="n">both_count</span><span class="p">,</span> <span class="n">none_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>          

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">tqdm</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">,</span> <span class="n">agentview_img</span><span class="p">,</span> <span class="n">agentview_img2</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">))):</span>
    <span class="n">uv1</span> <span class="o">=</span> <span class="n">ZEDA_LEFT_CAM</span><span class="p">.</span><span class="nf">project_point</span><span class="p">(</span><span class="n">pos</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">uv2</span> <span class="o">=</span> <span class="n">ZEDB_RIGHT_CAM</span><span class="p">.</span><span class="nf">project_point</span><span class="p">(</span><span class="n">pos</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="n">img1_draw</span> <span class="o">=</span> <span class="n">img1</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">img2_draw</span> <span class="o">=</span> <span class="n">img2</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

    <span class="n">inside1</span><span class="p">,</span> <span class="n">inside2</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img1_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img1_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">img1_draw</span><span class="p">,</span> <span class="p">(</span><span class="n">uv1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">thickness</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inside1</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">left_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img2_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img2_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">img2_draw</span><span class="p">,</span> <span class="p">(</span><span class="n">uv2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">thickness</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inside2</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">right_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># wrap
</span>    <span class="k">if</span> <span class="n">inside1</span> <span class="ow">and</span> <span class="n">inside2</span><span class="p">:</span>
        <span class="n">both_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">inside1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">inside2</span><span class="p">:</span>
        <span class="n">none_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">out1</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">agentview1_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">agentview2_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img1_draw</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>
    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="n">out2</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img2_draw</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[Frame </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">] saved → </span><span class="si">{</span><span class="n">out1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">out2</span><span class="si">}</span><span class="s"> | inside1=</span><span class="si">{</span><span class="n">inside1</span><span class="si">}</span><span class="s">, inside2=</span><span class="si">{</span><span class="n">inside2</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># statistic results
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">========== check and statistical results ==========</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">left detecting: </span><span class="si">{</span><span class="n">left_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">right detecting: </span><span class="si">{</span><span class="n">right_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">both detecting: </span><span class="si">{</span><span class="n">both_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">both not detecting: </span><span class="si">{</span><span class="n">none_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">total numbers:   </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Saved projections and images in </span><span class="sh">'</span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s">/</span><span class="sh">'"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="low-level-teleoperation-data">Low level Teleoperation Data</h3> <p>We record rosbag from various topics. Here is the list of topics we record. However, this will need further post-processing because all the topics are published at different frequncies.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>topics:
  - /franka_state_controller/franka_states
  - /franka_gripper/joint_states
  - /franka_state_controller/joint_states_desired
  - /franka_state_controller/O_T_EE
  - /franka_state_controller/joint_states
  - /cartesian_impedance_controller/desired_pose
  - /zedA/zed_node_A/left/image_rect_color 
  - /zedB/zed_node_B/left/image_rect_color 
</code></pre></div></div> <p>We first estimated the frequencies of all the topics and then used our sampling algorithm to resample at a fixed frequency, corresponding to the rate at which we want our policy controller to operate.</p> <div class="row mt-3"> <div class="col-sm text-center"> <strong>Before Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-480.webp 480w,/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-800.webp 800w,/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/cartesian_impedance_controller/desired_pose @ 50Hz</p> </div> <div class="col-sm text-center"> <strong>Before Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-480.webp 480w,/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-800.webp 800w,/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/franka_state_controller/O_T_EE @ 607Hz</p> </div> </div> <div class="row mt-4"> <div class="col-sm text-center"> <strong>After Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-480.webp 480w,/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-800.webp 800w,/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/cartesian_impedance_controller/desired_pose @ 13Hz</p> </div> <div class="col-sm text-center"> <strong>After Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-480.webp 480w,/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-800.webp 800w,/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/franka_state_controller/O_T_EE @ 13Hz</p> </div> </div> <div class="caption mt-2 text-center"> Frequencies of both topics are aligned after applying our sampling algorithm, from highly different original rates (50Hz vs 607Hz) to a unified 13Hz (hyperparameter). </div> <p><strong>Here is the pseudo code for our sampling algorithm which ensures equal observations from all topics:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Synchronize multiple topics to a target frequency
</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="nf">min_timestamp</span><span class="p">(</span><span class="n">topics</span><span class="p">)</span>
<span class="n">end_time</span>   <span class="o">=</span> <span class="nf">max_timestamp</span><span class="p">(</span><span class="n">topics</span><span class="p">)</span>

<span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">target_freq</span>
<span class="n">t</span>  <span class="o">=</span> <span class="n">start_time</span>

<span class="k">while</span> <span class="n">t</span> <span class="o">&lt;=</span> <span class="n">end_time</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="nf">select_message</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">timestamp</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># the msg from the topic which has the greatest timestamp, but timestamp is &lt;= t
</span>        <span class="n">topic_buffer</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="o">=</span> <span class="n">msg</span>

    <span class="n">combined_msgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">topic_buffer</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">+=</span> <span class="n">dt</span>
</code></pre></div></div> <p>You can find the sampler package here.<a href="https://github.com/AnshPrakash/MimicPlay/tree/main/sampler"><img src="https://img.shields.io/badge/GitHub-Sampler-blue?logo=github" alt="GitHub Repo"/></a></p> <p>Further, we transform the data into robomimic style hdf5 format <a href="https://github.com/AnshPrakash/MimicPlay/tree/main/rosbag2hdf5"><img src="https://img.shields.io/badge/GitHub-rosbag2hdf5-blue?logo=github" alt="GitHub Repo"/></a></p> <blockquote> <p>The final teleoperation dataset, formatted in <strong>robomimic style</strong>, is now ready to be used in the training pipeline.</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FILE_CONTENTS {
 group      /
 group      /data
 group      /data/demo_0
 dataset    /data/demo_0/actions
 group      /data/demo_0/obs
 dataset    /data/demo_0/obs/O_T_EE
 dataset    /data/demo_0/obs/back_camera
 dataset    /data/demo_0/obs/ee_pose
 dataset    /data/demo_0/obs/front_camera
 dataset    /data/demo_0/obs/gripper_joint_states
 dataset    /data/demo_0/obs/joint_states
 dataset    /data/demo_0/obs/joint_states_desired
 group      /data/demo_1
 dataset    /data/demo_1/actions
 group      /data/demo_1/obs
 dataset    /data/demo_1/obs/O_T_EE
 dataset    /data/demo_1/obs/back_camera
 dataset    /data/demo_1/obs/ee_pose
 dataset    /data/demo_1/obs/front_camera
 dataset    /data/demo_1/obs/gripper_joint_states
 dataset    /data/demo_1/obs/joint_states
 dataset    /data/demo_1/obs/joint_states_desired
 group      /mask
 dataset    /mask/train
}
</code></pre></div></div> <hr/> <div class="row mt-3"> <div class="col-sm text-center"> <strong>Method</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mimicplay/training-480.webp 480w,/assets/img/mimicplay/training-800.webp 800w,/assets/img/mimicplay/training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mimicplay/training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption mt-2 text-center"> Overview of MimicPlay <d-cite key="wang2023mimicplaylonghorizonimitationlearning"></d-cite> </div> <h2 id="high-level-latent-planner">High Level Latent Planner</h2> <h3 id="model">Model</h3> <p>With the collected human play data and the corresponding 3D hand trajectories ( \tau ), we formalize the latent plan learning problem as a <strong>goal-conditioned 3D trajectory generation task</strong>. In this formulation, the planner must generate feasible hand trajectories conditioned on the specified goal state.</p> <p>To model this distribution, we adopt a <strong>Gaussian Mixture Model (GMM)</strong> as the high-level planner. The GMM captures the multi-modal nature of human demonstrations, where multiple valid trajectories may exist for achieving the same goal. This provides several advantages:</p> <ul> <li><strong>Goal-conditioning</strong>: ensures that the generated trajectory is consistent with the task objective.</li> <li><strong>Flexibility</strong>: supports multiple valid solutions instead of collapsing to a single mode.</li> <li><strong>Robustness across tasks</strong>: enables the planner to generalize across diverse demonstrations collected from different tasks.</li> </ul> <p>In summary, the GMM-based planner learns to represent the distribution of goal-conditioned trajectories, which allows for generating diverse yet feasible high-level plans.</p> <h3 id="latent-plan">Latent plan</h3> <p>Our high-level planner is formulated as a <strong>latent plan generator</strong>.<br/> We use a pretrained <strong>GMM model</strong> to produce latent trajectory plans from the collected demonstrations.<br/> These latent plans are not directly executed by the robot but are instead passed to the <strong>low-level controller</strong>, which converts them into executable motor commands.<br/> This hierarchical setup defines the high-level component as a latent plan rather than direct control.</p> <h3 id="multi-modality">Multi-modality</h3> <p>The training model takes <strong>multi-modal inputs</strong> to construct the high-level planner.<br/> Specifically, it receives <strong>two-view RGB images</strong> together with the corresponding <strong>hand position information</strong> as inputs, and outputs a <strong>GMM trajectory distribution</strong>.<br/> This setup allows the model to learn from both visual context and motion data when generating latent plans.</p> <h3 id="training">Training</h3> <h4 id="setup">Setup</h4> <p>For the collected demonstration dataset, we used <strong>one demo as the validation set</strong>, while the remaining demos were used for <strong>training</strong>. The training was conducted following the <strong>configuration provided in the reference paper</strong>. For hyperparameters, we mainly relied on the <strong>default settings from the official repository</strong>, while performing <strong>additional tuning</strong> based on our own dataset to improve performance, e.g. “goal image range” and “std”.</p> <h4 id="evaluation">Evaluation</h4> <p>We evaluated the high-level planner using two metrics:</p> <ol> <li> <p><strong>GMM likelihood probability (training phase)</strong><br/> During training, we monitored the <strong>likelihood of the ground-truth data under the learned GMM model</strong>. This serves as a measure of how well the model captures the distribution of the demonstrations.</p> </li> <li> <p><strong>Distance error (test phase)</strong><br/> On the test prompts, we computed the <strong>distance error</strong> between the predicted trajectories and the ground-truth hand positions. Since our high-level planner is a <strong>probabilistic model</strong>, we performed <strong>multiple samples for each time step</strong> in the sequence. The final error metric was obtained by averaging across the entire video sequence and across all samples.</p> </li> </ol> <h2 id="low-level-policy">Low Level Policy</h2> <p>During <strong>training</strong>, the low-level policy receives a latent embedding of the robot’s trajectory from the high-level latent planner. This embedding provides rich contextual information, significantly reducing the need for large amounts of teleoperation data.</p> <p>During <strong>testing</strong>, the low-level policy instead receives a latent embedding of the human trajectory. This acts as a <em>human prompt</em>, guiding the robot to replicate the demonstrated actions. At the same time, the policy continuously collects observations from onboard cameras and proprioceptive signals (via ROS topics) at the desired frequency.</p> <p>Below is the pseudocode illustrating how the system acquires observations at a fixed frequency in the real robot setup:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Get observations at a desired frequency
</span>
<span class="c1"># 1. Compute how long we should wait between observations
</span><span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">target_frequency</span>

<span class="k">while</span> <span class="ow">not</span> <span class="nf">shutting_down</span><span class="p">():</span>
    <span class="c1"># 2. Wait until *all* topics have fresh data newer than last_obs_time + dt
</span>    <span class="k">if</span> <span class="nf">all_topics_ready</span><span class="p">(</span><span class="n">threshold_time</span><span class="o">=</span><span class="n">last_obs_time</span> <span class="o">+</span> <span class="n">dt</span><span class="p">):</span>
        
        <span class="c1"># 3. Snapshot the latest messages and timestamps
</span>        <span class="n">msgs</span><span class="p">,</span> <span class="n">times</span> <span class="o">=</span> <span class="nf">snapshot_latest_messages</span><span class="p">()</span>

        <span class="c1"># 4. Convert each message into a NumPy-friendly format
</span>        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="n">topic</span><span class="p">:</span> <span class="nf">convert_to_numpy</span><span class="p">(</span><span class="n">msgs</span><span class="p">[</span><span class="n">topic</span><span class="p">])</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">msgs</span><span class="p">}</span>

        <span class="c1"># 5. Update last observation time and return a dictionary
</span>        <span class="n">last_obs_time</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">times</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">timestamp</span><span class="sh">"</span><span class="p">:</span> <span class="n">last_obs_time</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">times</span><span class="sh">"</span><span class="p">:</span> <span class="n">times</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="c1"># 6. Otherwise, wait briefly and try again
</span>    <span class="nf">sleep_a_bit</span><span class="p">()</span>
</code></pre></div></div> <blockquote> <p>Actual code for reference here <a href="https://github.com/AnshPrakash/franka_teleop/blob/b088a9c38e2cb60ba15d4b1b7c3e7edeb2698313/scripts/policy_controller.py#L345"><img src="https://img.shields.io/badge/GitHub-PolicyController-blue?logo=github" alt="GitHub Repo"/></a></p> </blockquote> <p>In the original paper, the robot policy operated at 17 Hz. However, our ZED camera could capture observations at a maximum frequency of 14 Hz, which set the upper bound for our deployed policy. Ultimately, we chose to run the robot policy at 13 Hz.</p> <div class="row mt-3"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mimicplay/low-level-policy.drawio-480.webp 480w,/assets/img/mimicplay/low-level-policy.drawio-800.webp 800w,/assets/img/mimicplay/low-level-policy.drawio-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mimicplay/low-level-policy.drawio.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption mt-2 text-center"> The low-level policy receives a latent plan, image observations, and proprioceptive inputs, then samples an action from a multimodal Gaussian distribution. <d-cite key="wang2023mimicplaylonghorizonimitationlearning"></d-cite> </div> <h2 id="experiments">Experiments</h2> <h3 id="high-level-planner">High Level Planner</h3> <p>After completing the training of the high-level latent planner, we first collected <strong>video prompts</strong> and performed a <strong>visual inspection of the predicted trajectories</strong>. This step allowed us to qualitatively evaluate whether the generated trajectories aligned with the expected task goals and to compare them against the ground-truth trajectories from the demonstrations. Below we show example visualizations of the predicted trajectories。</p> <div class="row mt-4"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high_level/single_view/start_with_traj-480.webp 480w,/assets/img/high_level/single_view/start_with_traj-800.webp 800w,/assets/img/high_level/single_view/start_with_traj-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/high_level/single_view/start_with_traj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>current states of hand with 10 steps future trajectory</p> </div> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high_level/single_view/goal-480.webp 480w,/assets/img/high_level/single_view/goal-800.webp 800w,/assets/img/high_level/single_view/goal-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/high_level/single_view/goal.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>goal states of hand</p> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/high_level/single_view/traj_video.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption text-center"> trajectory through time steps </div> <h3 id="live-system-policy-controller">Live system: Policy Controller</h3> <p><a href="https://github.com/AnshPrakash/franka_teleop/blob/robot-policy/scripts/policy_controller.py"><img src="https://img.shields.io/badge/GitHub-PolicyController-blue?logo=github" alt="GitHub Repo"/></a></p> <p>Below we present our evaluation results for the low-level policy. Although the success rate was 0%, we have developed a solid understanding of the underlying reasons for this outcome.</p> <p>Here is our evaluation video results:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <h5>Human Prompts</h5> <figure> <video src="/assets/video/mimicplay/Human_prompts/data-2025-09-06_10-56-20/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <h5>Robot Policy Acting</h5> <figure> <video src="/assets/video/mimicplay/lowlevel-eval-policy_evaluation/robot-policy-eval-recordings/demo_0/data-2025-09-07_16-11-12/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/Human_prompts/data-2025-09-06_10-58-04/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/lowlevel-eval-policy_evaluation/robot-policy-eval-recordings/demo_2/data-2025-09-07_16-29-01/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/Human_prompts/data-2025-09-07_14-51-35_demo3/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/lowlevel-eval-policy_evaluation/robot-policy-eval-recordings/demo_3/data-2025-09-07_15-51-07/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Left: Human prompts, Right: Robot policy acting </div> <h3 id="key-limitations-observed">Key Limitations Observed</h3> <ol> <li> <p><strong>High-Level Planner — Poor Embedding Quality</strong></p> <ul> <li>We found that the high-level planner produced <strong>high prediction errors</strong> for trajectories, which resulted in <strong>poor latent embeddings</strong>.</li> <li>Through hyperparameter tuning, we discovered that our dataset required <strong>fewer modes</strong> for accurate trajectory prediction.</li> <li>Due to these weak embeddings, the low-level policy experienced <strong>high variance between similar trajectories</strong>, preventing it from fully leveraging the advantages of human guidance.</li> </ul> </li> <li> <p><strong>Absence of Wrist Camera</strong></p> <ul> <li>There was a significant <strong>distribution shift</strong> between training and evaluation image inputs from the front and back cameras.</li> <li>The original authors used a <strong>wrist-mounted camera</strong>, which helped stabilize the robot policy.</li> <li>Adding a wrist camera in our setup would likely <strong>reduce distribution shift</strong> and improve performance—<strong>provided that a robust latent embedding of the human prompt is available</strong>.</li> </ul> </li> </ol> <hr/> <h2 id="extension-to-bimanual-tiago">Extension to Bimanual Tiago</h2> <h3 id="update-to-hand-tracking-system-to-two-hands">Update to Hand Tracking system to two hands</h3> <p>The current pretrained hand detection model is able to distinguish between the <strong>left and right hands</strong>. However, since our setup only uses <strong>two calibrated camera views</strong>, the detection results can vary significantly. One major challenge arises when the <strong>two hands occlude each other</strong>, in which case it may be impossible to reliably observe both hands in both camera views at the same time. This directly limits our ability to obtain accurate <strong>3D hand position estimates</strong> through triangulation.</p> <p>To address this issue, one potential approach we are exploring is <strong>temporal interpolation</strong>. Specifically, when a hand temporarily disappears due to occlusion, we use its <strong>2D infomation before and after the disappearance</strong> to interpolate the missing frames. By filling in these occluded intervals, we aim to maintain more consistent 3D hand trajectory estimation for bimanual tasks.</p> <hr/> <h2 id="acknowledgements">Acknowledgements</h2> <p>We would like to thank our supervisor, <a href="https://pearl-lab.com/people/franziska-herbert/">Franziska Herbert</a>, for her guidance and support throughout this project. We also extend our gratitude to the course organizer and the lab staff for providing the resources and assistance that made this work possible. Finally, we thank the authors of <a href="https://mimic-play.github.io/"><strong>MimicPlay</strong></a> for making their code publicly available.</p> <hr/> <h3 id="bibtex">BibTeX</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">prakashzhou2025mimicplay</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Prakash, Ansh and Zhou, Xiaoqi}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{MimicPlay on Franka Arm and its Extension}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://anshprakash.github.io/blog/2025/mimicplay/}}</span><span class="p">,</span>
  <span class="na">note</span>         <span class="p">=</span> <span class="s">{IROBMAN Lab Blog}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/>]]></content><author><name>Ansh Prakash</name></author><category term="Imitation-Learning,"/><category term="Learning-from-Human,"/><category term="Long-Horizon-Manipulation,"/><category term="pearl-lab"/><summary type="html"><![CDATA[This blog is part of our university’s project lab, where we are working on replicating MimicPlay using a real one-arm robotic platform in our lab. Building on this setup, we aim to extend the approach to bi-manual systems such as the Tiago robot. Our work explores how abundant human play data can be leveraged to guide efficient low-level robot policies.]]></summary></entry><entry><title type="html">MimicPlay on Franka Arm and its Extension2</title><link href="https://anshprakash.github.io/blog/2025/mimicplay2/" rel="alternate" type="text/html" title="MimicPlay on Franka Arm and its Extension2"/><published>2025-09-08T00:00:00+00:00</published><updated>2025-09-08T00:00:00+00:00</updated><id>https://anshprakash.github.io/blog/2025/mimicplay2</id><content type="html" xml:base="https://anshprakash.github.io/blog/2025/mimicplay2/"><![CDATA[<h2 id="introduction">Introduction</h2> <hr/> <h2 id="related-works">Related Works</h2> <hr/> <h2 id="mimicplay">MimicPlay</h2> <hr/> <h2 id="franka-teleoperation-system">Franka Teleoperation system</h2> <p>We developed our own teleoperation system to collect low-level demonstration data. Using a Meta Quest VR controller, we operated the Panda arm, with the headset tracking the controller’s pose in real time. The pose differences from the controller were transformed into corresponding end-effector movements on the robot, enabling us to perform various pick-and-place tasks.</p> <p>We used a Cartesian impedance controller for safer operation and additionally calibrated gravity compensation for a different gripper. This ensures that the end-effector neither drops nor unintentionally lifts depending on the load. Instructions for calibration can be found <a href="https://github.com/nbfigueroa/franka_interactive_controllers/blob/main/doc/instructions/external_tool_compensation.md">here</a>.</p> <p>Here is the code for teleoperation: <a href="https://github.com/AnshPrakash/franka_teleop"><img src="https://img.shields.io/badge/GitHub-Franka--Teleop-blue?logo=github" alt="GitHub Repo"/></a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/teleop_demo.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/teleop_demo_front.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Here is a video of Teleoperation system in action </div> <hr/> <h2 id="data-collection-pipeline">Data Collection Pipeline</h2> <h3 id="human-play-data">Human Play data</h3> <p>We store the human play data in <strong>mp4 format</strong> with a frame rate of <strong>20 FPS</strong>. Afterwards, we apply some <strong>post-processing</strong> to convert it into the required <strong>robomimic format</strong>.</p> <ol> <li> <p><strong>Hand detection</strong><br/> We use a pretrained hand detection model<a href="https://github.com/ddshan/hand_object_detector"><img src="https://img.shields.io/badge/GitHub-handobj-blue?logo=github" alt="GitHub Repo"/></a> to locate human hands in the video frames. In total, we collected <strong>10 demonstrations</strong>. After filtering, we discarded several demos where the hands could not be reliably detected.</p> </li> <li> <p><strong>3D triangulation and dataset conversion</strong><br/> Using the <strong>calibrated stereo camera setup</strong> (two synchronized viewpoints), we triangulate the detected hand positions to obtain their <strong>3D coordinates in the world frame</strong>. These 3D hand trajectories are then converted into the <strong>robomimic dataset format</strong>.</p> </li> </ol> <p>Additionaly, we also do a <strong>Projection validation (visualization check)</strong> To verify the correctness of the calibration, we re-projected the obtained 3D points back to the image plane and visually inspected their alignment with the detected 2D hand positions. This ensured that the existed <strong>camera parameters</strong> were consistent with the real-world coordinate system. Below is the detection code used for this visualization check:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">buffer/Slow_version_Human_prompts_0</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># --- Load HDF5 ---
</span><span class="n">hdf5_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/home/xiaoqi/MimicPlay/mimicplay/datasets/playdata/Slow_version_Human_prompts/demo_0_new.hdf5</span><span class="sh">"</span>   <span class="c1"># update with your file path
</span><span class="k">with</span> <span class="n">h5py</span><span class="p">.</span><span class="nc">File</span><span class="p">(</span><span class="n">hdf5_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1"># Extract robot0 end-effector positions (605, 1, 3)
</span>    <span class="n">eef_pos</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/robot0_eef_pos</span><span class="sh">"</span><span class="p">][:]</span>  <span class="c1"># shape (605,1,3)
</span>    <span class="n">eef_pos</span> <span class="o">=</span> <span class="n">eef_pos</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># now (605, 3)
</span>
    <span class="c1"># Extract images if needed
</span>    <span class="n">agentview_img</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/agentview_image</span><span class="sh">"</span><span class="p">][:]</span> 
    <span class="n">agentview_img2</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/agentview_image_2</span><span class="sh">"</span><span class="p">][:]</span> 

<span class="c1"># --- Save raw 3D positions ---
</span><span class="n">np</span><span class="p">.</span><span class="nf">savetxt</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">robot0_eef_pos.txt</span><span class="sh">"</span><span class="p">),</span> <span class="n">eef_pos</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">"</span><span class="s">%.6f</span><span class="sh">"</span><span class="p">)</span>

<span class="n">ZEDA_LEFT_CAM</span> <span class="o">=</span> <span class="nc">CameraModel</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">=</span><span class="mf">1059.9764404296875</span><span class="p">,</span>
    <span class="n">fy</span><span class="o">=</span><span class="mf">1059.9764404296875</span><span class="p">,</span>
    <span class="n">cx</span><span class="o">=</span><span class="mf">963.07568359375</span><span class="p">,</span>
    <span class="n">cy</span><span class="o">=</span><span class="mf">522.3530883789062</span><span class="p">,</span>
    <span class="n">R_wc</span><span class="o">=</span><span class="n">R</span><span class="p">.</span><span class="nf">from_quat</span><span class="p">([</span><span class="o">-</span><span class="mf">0.404974467935380</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.808551385290863</span><span class="p">,</span> <span class="mf">0.425767747250020</span><span class="p">,</span> <span class="mf">0.031018753461827</span><span class="p">]).</span><span class="nf">as_matrix</span><span class="p">(),</span>
    <span class="n">t_wc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.903701253331141</span><span class="p">,</span> <span class="mf">0.444249176547482</span><span class="p">,</span> <span class="mf">0.598645500102408</span><span class="p">])</span>
<span class="p">)</span>

<span class="n">ZEDB_RIGHT_CAM</span> <span class="o">=</span> <span class="nc">CameraModel</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">=</span><span class="mf">1060.0899658203125</span><span class="p">,</span>
    <span class="n">fy</span><span class="o">=</span><span class="mf">1059.0899658203125</span><span class="p">,</span>
    <span class="n">cx</span><span class="o">=</span><span class="mf">958.9099731445312</span><span class="p">,</span>
    <span class="n">cy</span><span class="o">=</span><span class="mf">561.5670166015625</span><span class="p">,</span>
    <span class="n">R_wc</span><span class="o">=</span><span class="n">R</span><span class="p">.</span><span class="nf">from_quat</span><span class="p">([</span><span class="mf">0.81395177</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.40028226</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.07631803</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.41404371</span><span class="p">]).</span><span class="nf">as_matrix</span><span class="p">(),</span>
    <span class="n">t_wc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.11261126</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.52195948</span><span class="p">,</span> <span class="mf">0.55795671</span><span class="p">])</span>
<span class="p">)</span>

<span class="c1"># scale factor from 1920x1080 -&gt; 640x360
</span><span class="n">sx</span> <span class="o">=</span> <span class="mf">640.0</span> <span class="o">/</span> <span class="mf">1920.0</span>   <span class="c1"># = 1/3
</span><span class="n">sy</span> <span class="o">=</span> <span class="mf">360.0</span> <span class="o">/</span> <span class="mf">1080.0</span>   <span class="c1"># = 1/3
</span>
<span class="n">ZEDA_LEFT_CAM</span>  <span class="o">=</span> <span class="n">ZEDA_LEFT_CAM</span><span class="p">.</span><span class="nf">scaled</span><span class="p">(</span><span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>
<span class="n">ZEDB_RIGHT_CAM</span> <span class="o">=</span> <span class="n">ZEDB_RIGHT_CAM</span><span class="p">.</span><span class="nf">scaled</span><span class="p">(</span><span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>


<span class="c1"># --- Project and overlay ---
</span><span class="n">left_count</span><span class="p">,</span> <span class="n">right_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>         
<span class="n">both_count</span><span class="p">,</span> <span class="n">none_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>          

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">tqdm</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">,</span> <span class="n">agentview_img</span><span class="p">,</span> <span class="n">agentview_img2</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">))):</span>
    <span class="n">uv1</span> <span class="o">=</span> <span class="n">ZEDA_LEFT_CAM</span><span class="p">.</span><span class="nf">project_point</span><span class="p">(</span><span class="n">pos</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">uv2</span> <span class="o">=</span> <span class="n">ZEDB_RIGHT_CAM</span><span class="p">.</span><span class="nf">project_point</span><span class="p">(</span><span class="n">pos</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="n">img1_draw</span> <span class="o">=</span> <span class="n">img1</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">img2_draw</span> <span class="o">=</span> <span class="n">img2</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

    <span class="n">inside1</span><span class="p">,</span> <span class="n">inside2</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img1_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img1_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">img1_draw</span><span class="p">,</span> <span class="p">(</span><span class="n">uv1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">thickness</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inside1</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">left_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img2_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img2_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">img2_draw</span><span class="p">,</span> <span class="p">(</span><span class="n">uv2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">thickness</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inside2</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">right_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># wrap
</span>    <span class="k">if</span> <span class="n">inside1</span> <span class="ow">and</span> <span class="n">inside2</span><span class="p">:</span>
        <span class="n">both_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">inside1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">inside2</span><span class="p">:</span>
        <span class="n">none_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">out1</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">agentview1_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">agentview2_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img1_draw</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>
    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="n">out2</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img2_draw</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[Frame </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">] saved → </span><span class="si">{</span><span class="n">out1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">out2</span><span class="si">}</span><span class="s"> | inside1=</span><span class="si">{</span><span class="n">inside1</span><span class="si">}</span><span class="s">, inside2=</span><span class="si">{</span><span class="n">inside2</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># statistic results
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">========== check and statistical results ==========</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">left detecting: </span><span class="si">{</span><span class="n">left_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">right detecting: </span><span class="si">{</span><span class="n">right_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">both detecting: </span><span class="si">{</span><span class="n">both_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">both not detecting: </span><span class="si">{</span><span class="n">none_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">total numbers:   </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Saved projections and images in </span><span class="sh">'</span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s">/</span><span class="sh">'"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="low-level-teleoperation-data">Low level Teleoperation Data</h3> <p>We record rosbag from various topics. Here is the list of topics we record. However, this will need further post-processing because all the topics are published at different frequncies.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>topics:
  - /franka_state_controller/franka_states
  - /franka_gripper/joint_states
  - /franka_state_controller/joint_states_desired
  - /franka_state_controller/O_T_EE
  - /franka_state_controller/joint_states
  - /cartesian_impedance_controller/desired_pose
  - /zedA/zed_node_A/left/image_rect_color 
  - /zedB/zed_node_B/left/image_rect_color 
</code></pre></div></div> <p>We first estimated the frequencies of all the topics and then used our sampling algorithm to resample at a fixed frequency, corresponding to the rate at which we want our policy controller to operate.</p> <div class="row mt-3"> <div class="col-sm text-center"> <strong>Before Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-480.webp 480w,/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-800.webp 800w,/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/cartesian_impedance_controller/desired_pose @ 50Hz</p> </div> <div class="col-sm text-center"> <strong>Before Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-480.webp 480w,/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-800.webp 800w,/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/franka_state_controller/O_T_EE @ 607Hz</p> </div> </div> <div class="row mt-4"> <div class="col-sm text-center"> <strong>After Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-480.webp 480w,/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-800.webp 800w,/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/cartesian_impedance_controller/desired_pose @ 13Hz</p> </div> <div class="col-sm text-center"> <strong>After Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-480.webp 480w,/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-800.webp 800w,/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/franka_state_controller/O_T_EE @ 13Hz</p> </div> </div> <div class="caption mt-2 text-center"> Frequencies of both topics are aligned after applying our sampling algorithm, from highly different original rates (50Hz vs 607Hz) to a unified 13Hz (hyperparameter). </div> <p><strong>Here is the pseudo code for our sampling algorithm which ensures equal observations from all topics:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Synchronize multiple topics to a target frequency
</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="nf">min_timestamp</span><span class="p">(</span><span class="n">topics</span><span class="p">)</span>
<span class="n">end_time</span>   <span class="o">=</span> <span class="nf">max_timestamp</span><span class="p">(</span><span class="n">topics</span><span class="p">)</span>

<span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">target_freq</span>
<span class="n">t</span>  <span class="o">=</span> <span class="n">start_time</span>

<span class="k">while</span> <span class="n">t</span> <span class="o">&lt;=</span> <span class="n">end_time</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="nf">select_message</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">timestamp</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># the msg from the topic which has the greatest timestamp, but timestamp is &lt;= t
</span>        <span class="n">topic_buffer</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="o">=</span> <span class="n">msg</span>

    <span class="n">combined_msgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">topic_buffer</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">+=</span> <span class="n">dt</span>
</code></pre></div></div> <p>You can find the sampler package here.<a href="https://github.com/AnshPrakash/MimicPlay/tree/main/sampler"><img src="https://img.shields.io/badge/GitHub-Sampler-blue?logo=github" alt="GitHub Repo"/></a></p> <p>Further, we transform the data into robomimic style hdf5 format <a href="https://github.com/AnshPrakash/MimicPlay/tree/main/rosbag2hdf5"><img src="https://img.shields.io/badge/GitHub-rosbag2hdf5-blue?logo=github" alt="GitHub Repo"/></a></p> <blockquote> <p>The final teleoperation dataset, formatted in <strong>robomimic style</strong>, is now ready to be used in the training pipeline.</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FILE_CONTENTS {
 group      /
 group      /data
 group      /data/demo_0
 dataset    /data/demo_0/actions
 group      /data/demo_0/obs
 dataset    /data/demo_0/obs/O_T_EE
 dataset    /data/demo_0/obs/back_camera
 dataset    /data/demo_0/obs/ee_pose
 dataset    /data/demo_0/obs/front_camera
 dataset    /data/demo_0/obs/gripper_joint_states
 dataset    /data/demo_0/obs/joint_states
 dataset    /data/demo_0/obs/joint_states_desired
 group      /data/demo_1
 dataset    /data/demo_1/actions
 group      /data/demo_1/obs
 dataset    /data/demo_1/obs/O_T_EE
 dataset    /data/demo_1/obs/back_camera
 dataset    /data/demo_1/obs/ee_pose
 dataset    /data/demo_1/obs/front_camera
 dataset    /data/demo_1/obs/gripper_joint_states
 dataset    /data/demo_1/obs/joint_states
 dataset    /data/demo_1/obs/joint_states_desired
 group      /mask
 dataset    /mask/train
}
</code></pre></div></div> <hr/> <h2 id="high-level-latent-planner">High Level Latent Planner</h2> <h4 id="model">Model</h4> <p>With the collected human play data and the corresponding 3D hand trajectories ( \tau ), we formalize the latent plan learning problem as a <strong>goal-conditioned 3D trajectory generation task</strong>. In this formulation, the planner must generate feasible hand trajectories conditioned on the specified goal state.</p> <p>To model this distribution, we adopt a <strong>Gaussian Mixture Model (GMM)</strong> as the high-level planner. The GMM captures the multi-modal nature of human demonstrations, where multiple valid trajectories may exist for achieving the same goal. This provides several advantages:</p> <ul> <li><strong>Goal-conditioning</strong>: ensures that the generated trajectory is consistent with the task objective.</li> <li><strong>Flexibility</strong>: supports multiple valid solutions instead of collapsing to a single mode.</li> <li><strong>Robustness across tasks</strong>: enables the planner to generalize across diverse demonstrations collected from different tasks.</li> </ul> <p>In summary, the GMM-based planner learns to represent the distribution of goal-conditioned trajectories, which allows for generating diverse yet feasible high-level plans.</p> <h4 id="latent-plan">Latent plan</h4> <p>Our high-level planner is formulated as a <strong>latent plan generator</strong>.<br/> We use a pretrained <strong>GMM model</strong> to produce latent trajectory plans from the collected demonstrations.<br/> These latent plans are not directly executed by the robot but are instead passed to the <strong>low-level controller</strong>, which converts them into executable motor commands.<br/> This hierarchical setup defines the high-level component as a latent plan rather than direct control.</p> <h4 id="multi-modality">Multi-modality</h4> <p>The training model takes <strong>multi-modal inputs</strong> to construct the high-level planner.<br/> Specifically, it receives <strong>two-view RGB images</strong> together with the corresponding <strong>hand position information</strong> as inputs, and outputs a <strong>GMM trajectory distribution</strong>.<br/> This setup allows the model to learn from both visual context and motion data when generating latent plans.</p> <h4 id="training">Training</h4> <ul> <li> <p>Setup For the collected demonstration dataset, we used <strong>one demo as the validation set</strong>, while the remaining demos were used for <strong>training</strong>. The training was conducted following the <strong>configuration provided in the reference paper</strong>. For hyperparameters, we mainly relied on the <strong>default settings from the official repository</strong>, while performing <strong>additional tuning</strong> based on our own dataset to improve performance, e.g. “goal image range” and “std”.</p> </li> <li> <p>Evaluation We evaluated the high-level planner using two metrics:</p> </li> </ul> <ol> <li> <p><strong>GMM likelihood probability (training phase)</strong><br/> During training, we monitored the <strong>likelihood of the ground-truth data under the learned GMM model</strong>. This serves as a measure of how well the model captures the distribution of the demonstrations.</p> </li> <li> <p><strong>Distance error (test phase)</strong><br/> On the test prompts, we computed the <strong>distance error</strong> between the predicted trajectories and the ground-truth hand positions. Since our high-level planner is a <strong>probabilistic model</strong>, we performed <strong>multiple samples for each time step</strong> in the sequence. The final error metric was obtained by averaging across the entire video sequence and across all samples.</p> </li> </ol> <hr/> <h2 id="experiments">Experiments</h2> <h3 id="high-level-planner">High Level Planner</h3> <p>After completing the training of the high-level latent planner, we first collected <strong>video prompts</strong> and performed a <strong>visual inspection of the predicted trajectories</strong>. This step allowed us to qualitatively evaluate whether the generated trajectories aligned with the expected task goals and to compare them against the ground-truth trajectories from the demonstrations. Below we show example visualizations of the predicted trajectories。</p> <div class="row mt-4"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high_level/single_view/start_with_traj-480.webp 480w,/assets/img/high_level/single_view/start_with_traj-800.webp 800w,/assets/img/high_level/single_view/start_with_traj-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/high_level/single_view/start_with_traj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>current states of hand with 10 steps future trajectory</p> </div> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high_level/single_view/goal-480.webp 480w,/assets/img/high_level/single_view/goal-800.webp 800w,/assets/img/high_level/single_view/goal-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/high_level/single_view/goal.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>goal states of hand</p> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/high_level/single_view/traj_video.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption text-center"> trajectory through time steps </div> <hr/> <h2 id="extension-to-bimanual-tiago">Extension to Bimanual Tiago</h2> <h3 id="update-to-hand-tracking-system-to-two-hands">Update to Hand Tracking system to two hands</h3> <p>The current pretrained hand detection model is able to distinguish between the <strong>left and right hands</strong>. However, since our setup only uses <strong>two calibrated camera views</strong>, the detection results can vary significantly. One major challenge arises when the <strong>two hands occlude each other</strong>, in which case it may be impossible to reliably observe both hands in both camera views at the same time. This directly limits our ability to obtain accurate <strong>3D hand position estimates</strong> through triangulation.</p> <p>To address this issue, one potential approach we are exploring is <strong>temporal interpolation</strong>. Specifically, when a hand temporarily disappears due to occlusion, we use its <strong>2D infomation before and after the disappearance</strong> to interpolate the missing frames. By filling in these occluded intervals, we aim to maintain more consistent 3D hand trajectory estimation for bimanual tasks.</p> <hr/> <h2 id="acknowledgements">Acknowledgements</h2> <p>We would like to thank our supervisor, <a href="https://pearl-lab.com/people/franziska-herbert/">Franziska Herbert</a>, for her guidance and support throughout this project. We also extend our gratitude to the course organizer and the lab staff for providing the resources and assistance that made this work possible. Finally, we thank the authors of <a href="https://mimic-play.github.io/"><strong>MimicPlay</strong></a> for making their code publicly available.</p> <hr/> <h3 id="bibtex">BibTeX</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">prakashzhou2025mimicplay</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Prakash, Ansh and Zhou, Xiaoqi}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{MimicPlay on Franka Arm and its Extension}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://anshprakash.github.io/blog/2025/mimicplay/}}</span><span class="p">,</span>
  <span class="na">note</span>         <span class="p">=</span> <span class="s">{IROBMAN Lab Blog}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/>]]></content><author><name>Ansh Prakash</name></author><category term="Imitation-Learning,"/><category term="Learning-from-Human,"/><category term="Long-Horizon-Manipulation"/><summary type="html"><![CDATA[This blog is part of our university’s project lab, where we are working on replicating MimicPlay using a real one-arm robotic platform in our lab. Building on this setup, we aim to extend the approach to bi-manual systems such as the Tiago robot. Our work explores how abundant human play data can be leveraged to guide efficient low-level robot policies.]]></summary></entry></feed>