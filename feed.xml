<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://anshprakash.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://anshprakash.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-13T16:12:32+00:00</updated><id>https://anshprakash.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">MimicPlay on Franka Arm and its Extension</title><link href="https://anshprakash.github.io/blog/2025/mimicplay/" rel="alternate" type="text/html" title="MimicPlay on Franka Arm and its Extension"/><published>2025-09-08T00:00:00+00:00</published><updated>2025-09-08T00:00:00+00:00</updated><id>https://anshprakash.github.io/blog/2025/mimicplay</id><content type="html" xml:base="https://anshprakash.github.io/blog/2025/mimicplay/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Teaching robots to carry out general-purpose manipulation tasks efficiently has been a long-standing challenge. Recent advances in Imitation Learning (IL) have made notable progress toward this objective, particularly through supervised training with human teleoperation demonstrations or expert policy trajectories <d-cite key="pomerleau1988alvinn"> </d-cite> <d-cite key="zhang2018deep"> </d-cite> . Although promising, imitation learning has mostly been restricted to short-horizon skills, as collecting demonstrations for long-horizon, real-world tasks is time-consuming and labor-intensive. Two connected directions have emerged in recent literature to scale up imitation learning to complex long-horizon tasks: <em>hierarchical imitation learning</em> and <em>learning from play data</em>.</p> <ol> <li> <p><strong>Hierarchical imitation learning</strong> improves sample efficiency by breaking down end-to-end deep imitation learning into two stages: learning high-level planners and low-level visuomotor controllers <d-cite key="mandlekar2020learning"> </d-cite> <d-cite key="shiarlis2018taco"> </d-cite> .</p> </li> <li> <p><strong>Learning from play data</strong> uses a different type of robot training data known as play data <d-cite key="lynch2020play"> </d-cite>, which is collected via human-operated robots exploring their environment without explicit task instructions. Such data captures more diverse behaviors and situations than task-specific demonstrations <d-cite key="lynch2020play"> </d-cite> <d-cite key="cui2022play"> </d-cite>. Methods that leverage play data typically train hierarchical policies, where the high-level planner models intent and the low-level controllers handle goal-directed actions <d-cite key="lynch2020play"> </d-cite>. Nonetheless, collecting real-world play data is resource-intensive; for instance, C-BeT <d-cite key="cui2022play"> </d-cite> requires 4.5 hours of play data for manipulation skills in one scene, while TACO-RL <d-cite key="rosete2022latent"> </d-cite> needs 6 hours for a single 3D tabletop environment.</p> </li> </ol> <div class="row mt-3"> <div class="col-sm text-center" id="fig:humanplay-collection"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mimicplay/scale_data-480.webp 480w,/assets/img/mimicplay/scale_data-800.webp 800w,/assets/img/mimicplay/scale_data-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mimicplay/scale_data.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig1<d-cite key="wang2023mimicplaylonghorizonimitationlearning"></d-cite>: Humans can complete long-horizon tasks much faster than teleoperated robots. Inspired by this, MIMICPLAY<d-cite key="wang2023mimicplaylonghorizonimitationlearning"></d-cite> is implemented as a hierarchical imitation learning framework that learns high-level planning from inexpensive human play data and low-level control policies from a small set of multi-task teleoperated robot demonstrations. </div> <p>MimicPlay <d-cite key="wang2023mimicplaylonghorizonimitationlearning"></d-cite> suggests that data for learning both high-level planning and low-level control can take various forms, potentially lowering the cost of imitation learning for complex, long-horizon tasks.</p> <p>Building on this idea, the authors propose a learning paradigm where robots acquire high-level plans from human play data, in which humans freely interact with the environment using their hands. This type of data is faster and easier to gather than robot teleoperation data, enabling large-scale collection that captures a wide range of behaviors and scenarios <a href="#fig:humanplay-collection">Fig 1</a>.</p> <p>Subsequently, the robot learns low-level manipulation policies from a limited set of demonstrations collected via human teleoperation. While demonstration data is more expensive to obtain, it avoids the challenges arising from differences between human and robot embodiments.</p> <div class="row mt-3"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mimicplay/mimic-play-inspiration-480.webp 480w,/assets/img/mimicplay/mimic-play-inspiration-800.webp 800w,/assets/img/mimicplay/mimic-play-inspiration-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mimicplay/mimic-play-inspiration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mimicplay/mimicplay-fillgap-480.webp 480w,/assets/img/mimicplay/mimicplay-fillgap-800.webp 800w,/assets/img/mimicplay/mimicplay-fillgap-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mimicplay/mimicplay-fillgap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Human is able to complete a long-horizon task much faster than a teleoperated robot. This observation is the inspiration for MimicPlay, a hierarchical imitation learning algorithm that learns a high-level planner from cheap human play data and a low-level control policy from a small amount of multi-task teleoperated robot demonstrations. </div> <hr/> <h2 id="related-works">Related Works</h2> <p><strong>Imitation learning from demonstrations</strong>: Imitation Learning (IL) enables robots to perform various manipulation tasks <d-cite key="calinon2010learning"></d-cite>, <d-cite key="ijspeert2002movement"></d-cite> , <d-cite key="schaal1999imitation"></d-cite> , <d-cite key="kober2010imitation"></d-cite> , <d-cite key="englert2018manipulation"></d-cite> , <d-cite key="finn2017oneshot"></d-cite> , <d-cite key="billard2008rpd"></d-cite> , <d-cite key="argall2009survey"></d-cite> . Classical methods like DMP and PrMP <d-cite key="schaal2006dmp"></d-cite>, <d-cite key="kober2009primitives"></d-cite>, <d-cite key="paraschos2013promp"></d-cite>, <d-cite key="paraschos2018promp"></d-cite> are sample-efficient but limited with high-dimensional inputs and closed-loop control. Deep IL approaches <d-cite key="mandlekar2021offline"></d-cite>, <d-cite key="zhang2018deep"></d-cite>, <d-cite key="mandlekar2020learning"></d-cite>, <d-cite key="lynch2020grounding"></d-cite>, <d-cite key="reed2022gato"></d-cite>, <d-cite key="lee2022multimodal"></d-cite>, <d-cite key="shridhar2022cliport"></d-cite> offer more flexibility but require many human demonstrations, which is labor-intensive <d-cite key="jang2022bcz"></d-cite>, <d-cite key="shafiullah2022robosuite"></d-cite>. MimicPlay reduces this burden by leveraging easily collected human play data, minimizing the need for on-robot demonstrations.</p> <p><strong>Hierarchical imitation learning</strong> Previous methods for hierarchical policy learning relied solely on costly teleoperated robot demonstrations for both planning and control. In contrast, Mimicplay’s approach combines inexpensive human play data for high-level planning with a small amount of robot demonstrations for low-level control, improving planning ability while reducing data requirements.</p> <p><strong>Learning from human videos</strong> Previous work has explored using large-scale human video data to support robot policy learning , with approaches like R3M <d-cite key="tobin2017domain"></d-cite> and MVP <d-cite key="andrychowicz2020learning"></d-cite> leveraging the Ego4D dataset <d-cite key="akkaya2019solving"></d-cite> to pretrain visual representations. However, domain diversity makes transferring these features to specific manipulation tasks difficult, and even simple augmentations can be similarly effective <d-cite key="rahmatizadeh2018vision"></d-cite> . To reduce this gap, some methods use in-domain human videos, enabling sample-efficient reward shaping and imitation learning, though they mainly extract rewards or features rather than aiding low-level action generation. In contrast, Mimicplay’s work derives trajectory-level task plans from human play data, offering high-level guidance that improves low-level control in long-horizon manipulation tasks.</p> <p><strong>Learning from play data</strong> The proposed approach extends prior work on learning from play <d-cite key="yu2018one"></d-cite>, <d-cite key="lynch2020play"></d-cite>, <d-cite key="cui2022play"></d-cite> by replacing labor-intensive teleoperated play data with human play data collected through freehand interactions with the environment. This strategy provides rich trajectory-level guidance in only minutes, enabling robots to master complex long-horizon tasks with less than 30 minutes of teleoperation data.</p> <hr/> <h2 id="mimicplay">MimicPlay</h2> <h3 id="high-level-latent-planner">High Level Latent Planner</h3> <h3 id="model">Model</h3> <p>With the collected human play data and the corresponding 3D hand trajectories ( \tau ), we formalize the latent plan learning problem as a <strong>goal-conditioned 3D trajectory generation task</strong>. In this formulation, the planner must generate feasible hand trajectories conditioned on the specified goal state.</p> <p>To model this distribution, we adopt a <strong>Gaussian Mixture Model (GMM)</strong> as the high-level planner. The GMM captures the multi-modal nature of human demonstrations, where multiple valid trajectories may exist for achieving the same goal. This provides several advantages:</p> <ul> <li><strong>Goal-conditioning</strong>: ensures that the generated trajectory is consistent with the task objective.</li> <li><strong>Flexibility</strong>: supports multiple valid solutions instead of collapsing to a single mode.</li> <li><strong>Robustness across tasks</strong>: enables the planner to generalize across diverse demonstrations collected from different tasks.</li> </ul> <p>In summary, the GMM-based planner learns to represent the distribution of goal-conditioned trajectories, which allows for generating diverse yet feasible high-level plans.</p> <h3 id="latent-plan">Latent plan</h3> <p>Our high-level planner is formulated as a <strong>latent plan generator</strong>.<br/> We use a pretrained <strong>GMM model</strong> to produce latent trajectory plans from the collected demonstrations.<br/> These latent plans are not directly executed by the robot but are instead passed to the <strong>low-level controller</strong>, which converts them into executable motor commands.<br/> This hierarchical setup defines the high-level component as a latent plan rather than direct control.</p> <h3 id="multi-modality">Multi-modality</h3> <p>The training model takes <strong>multi-modal inputs</strong> to construct the high-level planner.<br/> Specifically, it receives <strong>two-view RGB images</strong> together with the corresponding <strong>hand position information</strong> as inputs, and outputs a <strong>GMM trajectory distribution</strong>.<br/> This setup allows the model to learn from both visual context and motion data when generating latent plans.</p> <h1 id="implementation">Implementation</h1> <h2 id="franka-teleoperation-system">Franka Teleoperation system</h2> <p>We developed our own teleoperation system to collect low-level demonstration data. Using a Meta Quest VR controller, we operated the Panda arm, with the headset tracking the controller’s pose in real time. The pose differences from the controller were transformed into corresponding end-effector movements on the robot, enabling us to perform various pick-and-place tasks.</p> <p>We used a Cartesian impedance controller for safer operation and additionally calibrated gravity compensation for a different gripper. This ensures that the end-effector neither drops nor unintentionally lifts depending on the load. Instructions for calibration can be found <a href="https://github.com/nbfigueroa/franka_interactive_controllers/blob/main/doc/instructions/external_tool_compensation.md">here</a>.</p> <p>Here is the code for teleoperation: <a href="https://github.com/AnshPrakash/franka_teleop"><img src="https://img.shields.io/badge/GitHub-Franka--Teleop-blue?logo=github" alt="GitHub Repo"/></a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/teleop_demo.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/teleop_demo_front.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Here is a video of Teleoperation system in action </div> <hr/> <h2 id="data-collection-pipeline">Data Collection Pipeline</h2> <h3 id="human-play-data">Human Play data</h3> <p>We store the human play data in <strong>mp4 format</strong> with a frame rate of <strong>20 FPS</strong>. Afterwards, we apply some <strong>post-processing</strong> to convert it into the required <strong>robomimic format</strong>.</p> <ol> <li> <p><strong>Hand detection</strong><br/> We use a pretrained hand detection model<a href="https://github.com/ddshan/hand_object_detector"><img src="https://img.shields.io/badge/GitHub-handobj-blue?logo=github" alt="GitHub Repo"/></a> to locate human hands in the video frames. In total, we collected <strong>10 demonstrations</strong>. After filtering, we discarded several demos where the hands could not be reliably detected.</p> </li> <li> <p><strong>3D triangulation and dataset conversion</strong><br/> Using the <strong>calibrated stereo camera setup</strong> (two synchronized viewpoints), we triangulate the detected hand positions to obtain their <strong>3D coordinates in the world frame</strong>. These 3D hand trajectories are then converted into the <strong>robomimic dataset format</strong>.</p> </li> </ol> <p>Additionaly, we also do a <strong>Projection validation (visualization check)</strong> To verify the correctness of the calibration, we re-projected the obtained 3D points back to the image plane and visually inspected their alignment with the detected 2D hand positions. This ensured that the existed <strong>camera parameters</strong> were consistent with the real-world coordinate system. Below is the detection code used for this visualization check:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">buffer/Slow_version_Human_prompts_0</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># --- Load HDF5 ---
</span><span class="n">hdf5_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/home/xiaoqi/MimicPlay/mimicplay/datasets/playdata/Slow_version_Human_prompts/demo_0_new.hdf5</span><span class="sh">"</span>   <span class="c1"># update with your file path
</span><span class="k">with</span> <span class="n">h5py</span><span class="p">.</span><span class="nc">File</span><span class="p">(</span><span class="n">hdf5_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1"># Extract robot0 end-effector positions (605, 1, 3)
</span>    <span class="n">eef_pos</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/robot0_eef_pos</span><span class="sh">"</span><span class="p">][:]</span>  <span class="c1"># shape (605,1,3)
</span>    <span class="n">eef_pos</span> <span class="o">=</span> <span class="n">eef_pos</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># now (605, 3)
</span>
    <span class="c1"># Extract images if needed
</span>    <span class="n">agentview_img</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/agentview_image</span><span class="sh">"</span><span class="p">][:]</span> 
    <span class="n">agentview_img2</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/agentview_image_2</span><span class="sh">"</span><span class="p">][:]</span> 

<span class="c1"># --- Save raw 3D positions ---
</span><span class="n">np</span><span class="p">.</span><span class="nf">savetxt</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">robot0_eef_pos.txt</span><span class="sh">"</span><span class="p">),</span> <span class="n">eef_pos</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">"</span><span class="s">%.6f</span><span class="sh">"</span><span class="p">)</span>

<span class="n">ZEDA_LEFT_CAM</span> <span class="o">=</span> <span class="nc">CameraModel</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">=</span><span class="mf">1059.9764404296875</span><span class="p">,</span>
    <span class="n">fy</span><span class="o">=</span><span class="mf">1059.9764404296875</span><span class="p">,</span>
    <span class="n">cx</span><span class="o">=</span><span class="mf">963.07568359375</span><span class="p">,</span>
    <span class="n">cy</span><span class="o">=</span><span class="mf">522.3530883789062</span><span class="p">,</span>
    <span class="n">R_wc</span><span class="o">=</span><span class="n">R</span><span class="p">.</span><span class="nf">from_quat</span><span class="p">([</span><span class="o">-</span><span class="mf">0.404974467935380</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.808551385290863</span><span class="p">,</span> <span class="mf">0.425767747250020</span><span class="p">,</span> <span class="mf">0.031018753461827</span><span class="p">]).</span><span class="nf">as_matrix</span><span class="p">(),</span>
    <span class="n">t_wc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.903701253331141</span><span class="p">,</span> <span class="mf">0.444249176547482</span><span class="p">,</span> <span class="mf">0.598645500102408</span><span class="p">])</span>
<span class="p">)</span>

<span class="n">ZEDB_RIGHT_CAM</span> <span class="o">=</span> <span class="nc">CameraModel</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">=</span><span class="mf">1060.0899658203125</span><span class="p">,</span>
    <span class="n">fy</span><span class="o">=</span><span class="mf">1059.0899658203125</span><span class="p">,</span>
    <span class="n">cx</span><span class="o">=</span><span class="mf">958.9099731445312</span><span class="p">,</span>
    <span class="n">cy</span><span class="o">=</span><span class="mf">561.5670166015625</span><span class="p">,</span>
    <span class="n">R_wc</span><span class="o">=</span><span class="n">R</span><span class="p">.</span><span class="nf">from_quat</span><span class="p">([</span><span class="mf">0.81395177</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.40028226</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.07631803</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.41404371</span><span class="p">]).</span><span class="nf">as_matrix</span><span class="p">(),</span>
    <span class="n">t_wc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.11261126</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.52195948</span><span class="p">,</span> <span class="mf">0.55795671</span><span class="p">])</span>
<span class="p">)</span>

<span class="c1"># scale factor from 1920x1080 -&gt; 640x360
</span><span class="n">sx</span> <span class="o">=</span> <span class="mf">640.0</span> <span class="o">/</span> <span class="mf">1920.0</span>   <span class="c1"># = 1/3
</span><span class="n">sy</span> <span class="o">=</span> <span class="mf">360.0</span> <span class="o">/</span> <span class="mf">1080.0</span>   <span class="c1"># = 1/3
</span>
<span class="n">ZEDA_LEFT_CAM</span>  <span class="o">=</span> <span class="n">ZEDA_LEFT_CAM</span><span class="p">.</span><span class="nf">scaled</span><span class="p">(</span><span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>
<span class="n">ZEDB_RIGHT_CAM</span> <span class="o">=</span> <span class="n">ZEDB_RIGHT_CAM</span><span class="p">.</span><span class="nf">scaled</span><span class="p">(</span><span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>


<span class="c1"># --- Project and overlay ---
</span><span class="n">left_count</span><span class="p">,</span> <span class="n">right_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>         
<span class="n">both_count</span><span class="p">,</span> <span class="n">none_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>          

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">tqdm</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">,</span> <span class="n">agentview_img</span><span class="p">,</span> <span class="n">agentview_img2</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">))):</span>
    <span class="n">uv1</span> <span class="o">=</span> <span class="n">ZEDA_LEFT_CAM</span><span class="p">.</span><span class="nf">project_point</span><span class="p">(</span><span class="n">pos</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">uv2</span> <span class="o">=</span> <span class="n">ZEDB_RIGHT_CAM</span><span class="p">.</span><span class="nf">project_point</span><span class="p">(</span><span class="n">pos</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="n">img1_draw</span> <span class="o">=</span> <span class="n">img1</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">img2_draw</span> <span class="o">=</span> <span class="n">img2</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

    <span class="n">inside1</span><span class="p">,</span> <span class="n">inside2</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img1_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img1_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">img1_draw</span><span class="p">,</span> <span class="p">(</span><span class="n">uv1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">thickness</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inside1</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">left_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img2_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img2_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">img2_draw</span><span class="p">,</span> <span class="p">(</span><span class="n">uv2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">thickness</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inside2</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">right_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># wrap
</span>    <span class="k">if</span> <span class="n">inside1</span> <span class="ow">and</span> <span class="n">inside2</span><span class="p">:</span>
        <span class="n">both_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">inside1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">inside2</span><span class="p">:</span>
        <span class="n">none_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">out1</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">agentview1_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">agentview2_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img1_draw</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>
    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="n">out2</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img2_draw</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[Frame </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">] saved → </span><span class="si">{</span><span class="n">out1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">out2</span><span class="si">}</span><span class="s"> | inside1=</span><span class="si">{</span><span class="n">inside1</span><span class="si">}</span><span class="s">, inside2=</span><span class="si">{</span><span class="n">inside2</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># statistic results
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">========== check and statistical results ==========</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">left detecting: </span><span class="si">{</span><span class="n">left_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">right detecting: </span><span class="si">{</span><span class="n">right_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">both detecting: </span><span class="si">{</span><span class="n">both_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">both not detecting: </span><span class="si">{</span><span class="n">none_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">total numbers:   </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Saved projections and images in </span><span class="sh">'</span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s">/</span><span class="sh">'"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="low-level-teleoperation-data">Low level Teleoperation Data</h3> <p>We record rosbag from various topics. Here is the list of topics we record. However, this will need further post-processing because all the topics are published at different frequncies.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>topics:
  - /franka_state_controller/franka_states
  - /franka_gripper/joint_states
  - /franka_state_controller/joint_states_desired
  - /franka_state_controller/O_T_EE
  - /franka_state_controller/joint_states
  - /cartesian_impedance_controller/desired_pose
  - /zedA/zed_node_A/left/image_rect_color 
  - /zedB/zed_node_B/left/image_rect_color 
</code></pre></div></div> <p>We first estimated the frequencies of all the topics and then used our sampling algorithm to resample at a fixed frequency, corresponding to the rate at which we want our policy controller to operate.</p> <div class="row mt-3"> <div class="col-sm text-center"> <strong>Before Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-480.webp 480w,/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-800.webp 800w,/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/cartesian_impedance_controller/desired_pose @ 50Hz</p> </div> <div class="col-sm text-center"> <strong>Before Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-480.webp 480w,/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-800.webp 800w,/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/franka_state_controller/O_T_EE @ 607Hz</p> </div> </div> <div class="row mt-4"> <div class="col-sm text-center"> <strong>After Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-480.webp 480w,/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-800.webp 800w,/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/cartesian_impedance_controller/desired_pose @ 13Hz</p> </div> <div class="col-sm text-center"> <strong>After Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-480.webp 480w,/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-800.webp 800w,/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/franka_state_controller/O_T_EE @ 13Hz</p> </div> </div> <div class="caption mt-2 text-center"> Frequencies of both topics are aligned after applying our sampling algorithm, from highly different original rates (50Hz vs 607Hz) to a unified 13Hz (hyperparameter). </div> <p><strong>Here is the pseudo code for our sampling algorithm which ensures equal observations from all topics:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Synchronize multiple topics to a target frequency
</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="nf">min_timestamp</span><span class="p">(</span><span class="n">topics</span><span class="p">)</span>
<span class="n">end_time</span>   <span class="o">=</span> <span class="nf">max_timestamp</span><span class="p">(</span><span class="n">topics</span><span class="p">)</span>

<span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">target_freq</span>
<span class="n">t</span>  <span class="o">=</span> <span class="n">start_time</span>

<span class="k">while</span> <span class="n">t</span> <span class="o">&lt;=</span> <span class="n">end_time</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="nf">select_message</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">timestamp</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># the msg from the topic which has the greatest timestamp, but timestamp is &lt;= t
</span>        <span class="n">topic_buffer</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="o">=</span> <span class="n">msg</span>

    <span class="n">combined_msgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">topic_buffer</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">+=</span> <span class="n">dt</span>
</code></pre></div></div> <p>You can find the sampler package here.<a href="https://github.com/AnshPrakash/MimicPlay/tree/main/sampler"><img src="https://img.shields.io/badge/GitHub-Sampler-blue?logo=github" alt="GitHub Repo"/></a></p> <p>Further, we transform the data into robomimic style hdf5 format <a href="https://github.com/AnshPrakash/MimicPlay/tree/main/rosbag2hdf5"><img src="https://img.shields.io/badge/GitHub-rosbag2hdf5-blue?logo=github" alt="GitHub Repo"/></a></p> <blockquote> <p>The final teleoperation dataset, formatted in <strong>robomimic style</strong>, is now ready to be used in the training pipeline.</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FILE_CONTENTS {
 group      /
 group      /data
 group      /data/demo_0
 dataset    /data/demo_0/actions
 group      /data/demo_0/obs
 dataset    /data/demo_0/obs/O_T_EE
 dataset    /data/demo_0/obs/back_camera
 dataset    /data/demo_0/obs/ee_pose
 dataset    /data/demo_0/obs/front_camera
 dataset    /data/demo_0/obs/gripper_joint_states
 dataset    /data/demo_0/obs/joint_states
 dataset    /data/demo_0/obs/joint_states_desired
 group      /data/demo_1
 dataset    /data/demo_1/actions
 group      /data/demo_1/obs
 dataset    /data/demo_1/obs/O_T_EE
 dataset    /data/demo_1/obs/back_camera
 dataset    /data/demo_1/obs/ee_pose
 dataset    /data/demo_1/obs/front_camera
 dataset    /data/demo_1/obs/gripper_joint_states
 dataset    /data/demo_1/obs/joint_states
 dataset    /data/demo_1/obs/joint_states_desired
 group      /mask
 dataset    /mask/train
}
</code></pre></div></div> <hr/> <div class="row mt-3"> <div class="col-sm text-left"> <strong>Method</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mimicplay/training-480.webp 480w,/assets/img/mimicplay/training-800.webp 800w,/assets/img/mimicplay/training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mimicplay/training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption mt-2 text-center"> Overview of MimicPlay <d-cite key="wang2023mimicplaylonghorizonimitationlearning"></d-cite> </div> <h2 id="high-level-latent-planner-1">High Level Latent Planner</h2> <h3 id="training">Training</h3> <h4 id="setup">Setup</h4> <p>For the collected demonstration dataset, we used <strong>one demo as the validation set</strong>, while the remaining demos were used for <strong>training</strong>. The training was conducted following the <strong>configuration provided in the reference paper</strong>. For hyperparameters, we mainly relied on the <strong>default settings from the official repository</strong>, while performing <strong>additional tuning</strong> based on our own dataset to improve performance, e.g. “goal image range” and “std”.</p> <h4 id="evaluation">Evaluation</h4> <p>We evaluated the high-level planner using two metrics:</p> <ol> <li> <p><strong>GMM likelihood probability (training phase)</strong><br/> During training, we monitored the <strong>likelihood of the ground-truth data under the learned GMM model</strong>. This serves as a measure of how well the model captures the distribution of the demonstrations.</p> </li> <li> <p><strong>Distance error (test phase)</strong><br/> On the test prompts, we computed the <strong>distance error</strong> between the predicted trajectories and the ground-truth hand positions. Since our high-level planner is a <strong>probabilistic model</strong>, we performed <strong>multiple samples for each time step</strong> in the sequence. The final error metric was obtained by averaging across the entire video sequence and across all samples.</p> </li> </ol> <h2 id="low-level-policy">Low Level Policy</h2> <p>During <strong>training</strong>, the low-level policy receives a latent embedding of the robot’s trajectory from the high-level latent planner. This embedding provides rich contextual information, significantly reducing the need for large amounts of teleoperation data.</p> <p>Additionally, we used <code class="language-plaintext highlighter-rouge">negative log likelihood</code> loss for training the models.</p> <div class="row mt-3"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mimicplay/low-level-training-480.webp 480w,/assets/img/mimicplay/low-level-training-800.webp 800w,/assets/img/mimicplay/low-level-training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mimicplay/low-level-training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Loss curve for training low-level policy</p> </div> </div> <div class="caption mt-2 text-center"> Divergence between validation and training loss after epoch 16 occurs due to the poor performance of the high-level planner, which was unable to generalize well across similar trajectories. </div> <p>During <strong>testing</strong>, the low-level policy instead receives a latent embedding of the human trajectory. This acts as a <em>human prompt</em>, guiding the robot to replicate the demonstrated actions. At the same time, the policy continuously collects observations from onboard cameras and proprioceptive signals (via ROS topics) at the desired frequency.</p> <p>Below is the pseudocode illustrating how the system acquires observations at a fixed frequency in the real robot setup:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Get observations at a desired frequency
</span>
<span class="c1"># 1. Compute how long we should wait between observations
</span><span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">target_frequency</span>

<span class="k">while</span> <span class="ow">not</span> <span class="nf">shutting_down</span><span class="p">():</span>
    <span class="c1"># 2. Wait until *all* topics have fresh data newer than last_obs_time + dt
</span>    <span class="k">if</span> <span class="nf">all_topics_ready</span><span class="p">(</span><span class="n">threshold_time</span><span class="o">=</span><span class="n">last_obs_time</span> <span class="o">+</span> <span class="n">dt</span><span class="p">):</span>
        
        <span class="c1"># 3. Snapshot the latest messages and timestamps
</span>        <span class="n">msgs</span><span class="p">,</span> <span class="n">times</span> <span class="o">=</span> <span class="nf">snapshot_latest_messages</span><span class="p">()</span>

        <span class="c1"># 4. Convert each message into a NumPy-friendly format
</span>        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="n">topic</span><span class="p">:</span> <span class="nf">convert_to_numpy</span><span class="p">(</span><span class="n">msgs</span><span class="p">[</span><span class="n">topic</span><span class="p">])</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">msgs</span><span class="p">}</span>

        <span class="c1"># 5. Update last observation time and return a dictionary
</span>        <span class="n">last_obs_time</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">times</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">timestamp</span><span class="sh">"</span><span class="p">:</span> <span class="n">last_obs_time</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">times</span><span class="sh">"</span><span class="p">:</span> <span class="n">times</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="c1"># 6. Otherwise, wait briefly and try again
</span>    <span class="nf">sleep_a_bit</span><span class="p">()</span>
</code></pre></div></div> <blockquote> <p>Actual code for reference here <a href="https://github.com/AnshPrakash/franka_teleop/blob/b088a9c38e2cb60ba15d4b1b7c3e7edeb2698313/scripts/policy_controller.py#L345"><img src="https://img.shields.io/badge/GitHub-PolicyController-blue?logo=github" alt="GitHub Repo"/></a></p> </blockquote> <p>In the original paper, the robot policy operated at 17 Hz. However, our ZED camera could capture observations at a maximum frequency of 14 Hz, which set the upper bound for our deployed policy. Ultimately, we chose to run the robot policy at 13 Hz.</p> <div class="row mt-3"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mimicplay/low-level-policy.drawio-480.webp 480w,/assets/img/mimicplay/low-level-policy.drawio-800.webp 800w,/assets/img/mimicplay/low-level-policy.drawio-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mimicplay/low-level-policy.drawio.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption mt-2 text-center"> The low-level policy receives a latent plan, image observations, and proprioceptive inputs, then samples an action from a multimodal Gaussian distribution. <d-cite key="wang2023mimicplaylonghorizonimitationlearning"></d-cite> </div> <h1 id="differences-in-the-original-setup-and-our-setup">Differences in the original Setup and our Setup</h1> <h2 id="experiments">Experiments</h2> <h3 id="high-level-planner">High Level Planner</h3> <p>After completing the training of the high-level latent planner, we first collected <strong>video prompts</strong> and performed a <strong>visual inspection of the predicted trajectories</strong>. This step allowed us to qualitatively evaluate whether the generated trajectories aligned with the expected task goals and to compare them against the ground-truth trajectories from the demonstrations. Below we show example visualizations of the predicted trajectories。</p> <div class="row mt-4"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high_level/single_view/start_with_traj-480.webp 480w,/assets/img/high_level/single_view/start_with_traj-800.webp 800w,/assets/img/high_level/single_view/start_with_traj-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/high_level/single_view/start_with_traj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>current states of hand with 10 steps future trajectory</p> </div> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high_level/single_view/goal-480.webp 480w,/assets/img/high_level/single_view/goal-800.webp 800w,/assets/img/high_level/single_view/goal-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/high_level/single_view/goal.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>goal states of hand</p> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/high_level/single_view/traj_video.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption text-center"> trajectory through time steps </div> <h3 id="low-level-policy--policy-controller-live-system">Low-Level Policy — Policy Controller (Live System)</h3> <p><a href="https://github.com/AnshPrakash/franka_teleop/blob/robot-policy/scripts/policy_controller.py"><img src="https://img.shields.io/badge/GitHub-PolicyController-blue?logo=github" alt="GitHub Repo"/></a></p> <p>Below we present our evaluation results for the low-level policy. Although the success rate was 0%, we have developed a solid understanding of the underlying reasons for this outcome.</p> <p>Here is our evaluation video results:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <h5>Human Prompts</h5> <figure> <video src="/assets/video/mimicplay/Human_prompts/data-2025-09-06_10-56-20/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <h5>Robot Policy Acting</h5> <figure> <video src="/assets/video/mimicplay/lowlevel-eval-policy_evaluation/robot-policy-eval-recordings/demo_0/data-2025-09-07_16-11-12/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/Human_prompts/data-2025-09-06_10-58-04/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/lowlevel-eval-policy_evaluation/robot-policy-eval-recordings/demo_2/data-2025-09-07_16-29-01/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/Human_prompts/data-2025-09-07_14-51-35_demo3/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/lowlevel-eval-policy_evaluation/robot-policy-eval-recordings/demo_3/data-2025-09-07_15-51-07/zedA_zed_node_A_left_image_rect_color.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Left: Human prompts, Right: Robot policy acting </div> <h3 id="key-limitations-observed">Key Limitations Observed</h3> <ol> <li> <p><strong>High-Level Planner — Poor Embedding Quality</strong></p> <ul> <li>We found that the high-level planner produced <strong>high prediction errors</strong> for trajectories, which resulted in <strong>poor latent embeddings</strong>.</li> <li>Through hyperparameter tuning, we discovered that our dataset required <strong>fewer modes</strong> for accurate trajectory prediction.</li> <li>Due to these weak embeddings, the low-level policy experienced <strong>high variance between similar trajectories</strong>, preventing it from fully leveraging the advantages of human guidance.</li> </ul> </li> <li> <p><strong>Absence of Wrist Camera</strong></p> <ul> <li>There was a significant <strong>distribution shift</strong> between training and evaluation image inputs from the front and back cameras.</li> <li>The original authors used a <strong>wrist-mounted camera</strong>, which helped stabilize the robot policy.</li> <li>Adding a wrist camera in our setup would likely <strong>reduce distribution shift</strong> and improve performance—<strong>provided that a robust latent embedding of the human prompt is available</strong>.</li> </ul> </li> </ol> <hr/> <h2 id="extension-to-bimanual-tiago--future-work">Extension to Bimanual Tiago — Future Work</h2> <h3 id="update-to-hand-tracking-system-to-two-hands">Update to Hand Tracking system to two hands</h3> <p>The current pretrained hand detection model is able to distinguish between the <strong>left and right hands</strong>. However, since our setup only uses <strong>two calibrated camera views</strong>, the detection results can vary significantly. One major challenge arises when the <strong>two hands occlude each other</strong>, in which case it may be impossible to reliably observe both hands in both camera views at the same time. This directly limits our ability to obtain accurate <strong>3D hand position estimates</strong> through triangulation.</p> <p>To address this issue, one potential approach we are exploring is <strong>temporal interpolation</strong>. Specifically, when a hand temporarily disappears due to occlusion, we use its <strong>2D infomation before and after the disappearance</strong> to interpolate the missing frames. By filling in these occluded intervals, we aim to maintain more consistent 3D hand trajectory estimation for bimanual tasks.</p> <p>We can use a Kalman filter to estimate the position of the occluded part by modeling the trajectory of the hand with a simple linear dynamics model.</p> <h3 id="high-level-planner--low-level-planner---bimanual">High-level planner &amp; Low-level planner - Bimanual</h3> <p>Only minor changes to the model are required to enable it for a bimanual scenario. Specifically, the action dimension needs to be doubled to account for the additional arm, and more observations must be added to track the positions of both end-effectors. The more challenging aspect lies in fine-tuning hyperparameters—such as the number of modes in the GMM decoder of the high-level planner—since data multimodality increases with two arms.</p> <hr/> <h1 id="conclusion">Conclusion</h1> <hr/> <h2 id="acknowledgements">Acknowledgements</h2> <p>We would like to thank our supervisor, <a href="https://pearl-lab.com/people/franziska-herbert/">Franziska Herbert</a>, for her guidance and support throughout this project. We also extend our gratitude to the course organizer and the lab staff for providing the resources and assistance that made this work possible. Finally, we thank the authors of <a href="https://mimic-play.github.io/"><strong>MimicPlay</strong></a> for making their code publicly available.</p> <hr/> <h3 id="bibtex">BibTeX</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">prakashzhou2025mimicplay</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Prakash, Ansh and Zhou, Xiaoqi}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{MimicPlay on Franka Arm and its Extension}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://anshprakash.github.io/blog/2025/mimicplay/}}</span><span class="p">,</span>
  <span class="na">note</span>         <span class="p">=</span> <span class="s">{IROBMAN Lab Blog}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/>]]></content><author><name>Ansh Prakash</name></author><category term="Imitation-Learning,"/><category term="Learning-from-Human,"/><category term="Long-Horizon-Manipulation,"/><category term="pearl-lab"/><summary type="html"><![CDATA[This blog is part of our university’s project lab, where we are working on replicating MimicPlay using a real one-arm robotic platform in our lab. Building on this setup, we aim to extend the approach to bi-manual systems such as the Tiago robot. Our work explores how abundant human play data can be leveraged to guide efficient low-level robot policies.]]></summary></entry><entry><title type="html">MimicPlay on Franka Arm and its Extension2</title><link href="https://anshprakash.github.io/blog/2025/mimicplay2/" rel="alternate" type="text/html" title="MimicPlay on Franka Arm and its Extension2"/><published>2025-09-08T00:00:00+00:00</published><updated>2025-09-08T00:00:00+00:00</updated><id>https://anshprakash.github.io/blog/2025/mimicplay2</id><content type="html" xml:base="https://anshprakash.github.io/blog/2025/mimicplay2/"><![CDATA[<h2 id="introduction">Introduction</h2> <hr/> <h2 id="related-works">Related Works</h2> <hr/> <h2 id="mimicplay">MimicPlay</h2> <hr/> <h2 id="franka-teleoperation-system">Franka Teleoperation system</h2> <p>We developed our own teleoperation system to collect low-level demonstration data. Using a Meta Quest VR controller, we operated the Panda arm, with the headset tracking the controller’s pose in real time. The pose differences from the controller were transformed into corresponding end-effector movements on the robot, enabling us to perform various pick-and-place tasks.</p> <p>We used a Cartesian impedance controller for safer operation and additionally calibrated gravity compensation for a different gripper. This ensures that the end-effector neither drops nor unintentionally lifts depending on the load. Instructions for calibration can be found <a href="https://github.com/nbfigueroa/franka_interactive_controllers/blob/main/doc/instructions/external_tool_compensation.md">here</a>.</p> <p>Here is the code for teleoperation: <a href="https://github.com/AnshPrakash/franka_teleop"><img src="https://img.shields.io/badge/GitHub-Franka--Teleop-blue?logo=github" alt="GitHub Repo"/></a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/teleop_demo.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/mimicplay/teleop_demo_front.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Here is a video of Teleoperation system in action </div> <hr/> <h2 id="data-collection-pipeline">Data Collection Pipeline</h2> <h3 id="human-play-data">Human Play data</h3> <p>We store the human play data in <strong>mp4 format</strong> with a frame rate of <strong>20 FPS</strong>. Afterwards, we apply some <strong>post-processing</strong> to convert it into the required <strong>robomimic format</strong>.</p> <ol> <li> <p><strong>Hand detection</strong><br/> We use a pretrained hand detection model<a href="https://github.com/ddshan/hand_object_detector"><img src="https://img.shields.io/badge/GitHub-handobj-blue?logo=github" alt="GitHub Repo"/></a> to locate human hands in the video frames. In total, we collected <strong>10 demonstrations</strong>. After filtering, we discarded several demos where the hands could not be reliably detected.</p> </li> <li> <p><strong>3D triangulation and dataset conversion</strong><br/> Using the <strong>calibrated stereo camera setup</strong> (two synchronized viewpoints), we triangulate the detected hand positions to obtain their <strong>3D coordinates in the world frame</strong>. These 3D hand trajectories are then converted into the <strong>robomimic dataset format</strong>.</p> </li> </ol> <p>Additionaly, we also do a <strong>Projection validation (visualization check)</strong> To verify the correctness of the calibration, we re-projected the obtained 3D points back to the image plane and visually inspected their alignment with the detected 2D hand positions. This ensured that the existed <strong>camera parameters</strong> were consistent with the real-world coordinate system. Below is the detection code used for this visualization check:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">buffer/Slow_version_Human_prompts_0</span><span class="sh">"</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># --- Load HDF5 ---
</span><span class="n">hdf5_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/home/xiaoqi/MimicPlay/mimicplay/datasets/playdata/Slow_version_Human_prompts/demo_0_new.hdf5</span><span class="sh">"</span>   <span class="c1"># update with your file path
</span><span class="k">with</span> <span class="n">h5py</span><span class="p">.</span><span class="nc">File</span><span class="p">(</span><span class="n">hdf5_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1"># Extract robot0 end-effector positions (605, 1, 3)
</span>    <span class="n">eef_pos</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/robot0_eef_pos</span><span class="sh">"</span><span class="p">][:]</span>  <span class="c1"># shape (605,1,3)
</span>    <span class="n">eef_pos</span> <span class="o">=</span> <span class="n">eef_pos</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># now (605, 3)
</span>
    <span class="c1"># Extract images if needed
</span>    <span class="n">agentview_img</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/agentview_image</span><span class="sh">"</span><span class="p">][:]</span> 
    <span class="n">agentview_img2</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="sh">"</span><span class="s">data/demo_0/obs/agentview_image_2</span><span class="sh">"</span><span class="p">][:]</span> 

<span class="c1"># --- Save raw 3D positions ---
</span><span class="n">np</span><span class="p">.</span><span class="nf">savetxt</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">robot0_eef_pos.txt</span><span class="sh">"</span><span class="p">),</span> <span class="n">eef_pos</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">"</span><span class="s">%.6f</span><span class="sh">"</span><span class="p">)</span>

<span class="n">ZEDA_LEFT_CAM</span> <span class="o">=</span> <span class="nc">CameraModel</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">=</span><span class="mf">1059.9764404296875</span><span class="p">,</span>
    <span class="n">fy</span><span class="o">=</span><span class="mf">1059.9764404296875</span><span class="p">,</span>
    <span class="n">cx</span><span class="o">=</span><span class="mf">963.07568359375</span><span class="p">,</span>
    <span class="n">cy</span><span class="o">=</span><span class="mf">522.3530883789062</span><span class="p">,</span>
    <span class="n">R_wc</span><span class="o">=</span><span class="n">R</span><span class="p">.</span><span class="nf">from_quat</span><span class="p">([</span><span class="o">-</span><span class="mf">0.404974467935380</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.808551385290863</span><span class="p">,</span> <span class="mf">0.425767747250020</span><span class="p">,</span> <span class="mf">0.031018753461827</span><span class="p">]).</span><span class="nf">as_matrix</span><span class="p">(),</span>
    <span class="n">t_wc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.903701253331141</span><span class="p">,</span> <span class="mf">0.444249176547482</span><span class="p">,</span> <span class="mf">0.598645500102408</span><span class="p">])</span>
<span class="p">)</span>

<span class="n">ZEDB_RIGHT_CAM</span> <span class="o">=</span> <span class="nc">CameraModel</span><span class="p">(</span>
    <span class="n">fx</span><span class="o">=</span><span class="mf">1060.0899658203125</span><span class="p">,</span>
    <span class="n">fy</span><span class="o">=</span><span class="mf">1059.0899658203125</span><span class="p">,</span>
    <span class="n">cx</span><span class="o">=</span><span class="mf">958.9099731445312</span><span class="p">,</span>
    <span class="n">cy</span><span class="o">=</span><span class="mf">561.5670166015625</span><span class="p">,</span>
    <span class="n">R_wc</span><span class="o">=</span><span class="n">R</span><span class="p">.</span><span class="nf">from_quat</span><span class="p">([</span><span class="mf">0.81395177</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.40028226</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.07631803</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.41404371</span><span class="p">]).</span><span class="nf">as_matrix</span><span class="p">(),</span>
    <span class="n">t_wc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.11261126</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.52195948</span><span class="p">,</span> <span class="mf">0.55795671</span><span class="p">])</span>
<span class="p">)</span>

<span class="c1"># scale factor from 1920x1080 -&gt; 640x360
</span><span class="n">sx</span> <span class="o">=</span> <span class="mf">640.0</span> <span class="o">/</span> <span class="mf">1920.0</span>   <span class="c1"># = 1/3
</span><span class="n">sy</span> <span class="o">=</span> <span class="mf">360.0</span> <span class="o">/</span> <span class="mf">1080.0</span>   <span class="c1"># = 1/3
</span>
<span class="n">ZEDA_LEFT_CAM</span>  <span class="o">=</span> <span class="n">ZEDA_LEFT_CAM</span><span class="p">.</span><span class="nf">scaled</span><span class="p">(</span><span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>
<span class="n">ZEDB_RIGHT_CAM</span> <span class="o">=</span> <span class="n">ZEDB_RIGHT_CAM</span><span class="p">.</span><span class="nf">scaled</span><span class="p">(</span><span class="n">sx</span><span class="p">,</span> <span class="n">sy</span><span class="p">)</span>


<span class="c1"># --- Project and overlay ---
</span><span class="n">left_count</span><span class="p">,</span> <span class="n">right_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>         
<span class="n">both_count</span><span class="p">,</span> <span class="n">none_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>          

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">tqdm</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">,</span> <span class="n">agentview_img</span><span class="p">,</span> <span class="n">agentview_img2</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">))):</span>
    <span class="n">uv1</span> <span class="o">=</span> <span class="n">ZEDA_LEFT_CAM</span><span class="p">.</span><span class="nf">project_point</span><span class="p">(</span><span class="n">pos</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">uv2</span> <span class="o">=</span> <span class="n">ZEDB_RIGHT_CAM</span><span class="p">.</span><span class="nf">project_point</span><span class="p">(</span><span class="n">pos</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="n">img1_draw</span> <span class="o">=</span> <span class="n">img1</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">img2_draw</span> <span class="o">=</span> <span class="n">img2</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

    <span class="n">inside1</span><span class="p">,</span> <span class="n">inside2</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img1_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img1_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">img1_draw</span><span class="p">,</span> <span class="p">(</span><span class="n">uv1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">uv1</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">thickness</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inside1</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">left_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img2_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">img2_draw</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">cv2</span><span class="p">.</span><span class="nf">circle</span><span class="p">(</span><span class="n">img2_draw</span><span class="p">,</span> <span class="p">(</span><span class="n">uv2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">uv2</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">thickness</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inside2</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">right_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># wrap
</span>    <span class="k">if</span> <span class="n">inside1</span> <span class="ow">and</span> <span class="n">inside2</span><span class="p">:</span>
        <span class="n">both_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">inside1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">inside2</span><span class="p">:</span>
        <span class="n">none_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">out1</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">agentview1_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">agentview2_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="s">.png</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img1_draw</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>
    <span class="n">cv2</span><span class="p">.</span><span class="nf">imwrite</span><span class="p">(</span><span class="n">out2</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img2_draw</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">))</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[Frame </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">] saved → </span><span class="si">{</span><span class="n">out1</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">out2</span><span class="si">}</span><span class="s"> | inside1=</span><span class="si">{</span><span class="n">inside1</span><span class="si">}</span><span class="s">, inside2=</span><span class="si">{</span><span class="n">inside2</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># statistic results
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">========== check and statistical results ==========</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">left detecting: </span><span class="si">{</span><span class="n">left_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">right detecting: </span><span class="si">{</span><span class="n">right_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">both detecting: </span><span class="si">{</span><span class="n">both_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">both not detecting: </span><span class="si">{</span><span class="n">none_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">total numbers:   </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">eef_pos</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Saved projections and images in </span><span class="sh">'</span><span class="si">{</span><span class="n">out_dir</span><span class="si">}</span><span class="s">/</span><span class="sh">'"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="low-level-teleoperation-data">Low level Teleoperation Data</h3> <p>We record rosbag from various topics. Here is the list of topics we record. However, this will need further post-processing because all the topics are published at different frequncies.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>topics:
  - /franka_state_controller/franka_states
  - /franka_gripper/joint_states
  - /franka_state_controller/joint_states_desired
  - /franka_state_controller/O_T_EE
  - /franka_state_controller/joint_states
  - /cartesian_impedance_controller/desired_pose
  - /zedA/zed_node_A/left/image_rect_color 
  - /zedB/zed_node_B/left/image_rect_color 
</code></pre></div></div> <p>We first estimated the frequencies of all the topics and then used our sampling algorithm to resample at a fixed frequency, corresponding to the rate at which we want our policy controller to operate.</p> <div class="row mt-3"> <div class="col-sm text-center"> <strong>Before Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-480.webp 480w,/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-800.webp 800w,/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/preprocessed_freq/cartesian_impedance_controller_desired_pose_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/cartesian_impedance_controller/desired_pose @ 50Hz</p> </div> <div class="col-sm text-center"> <strong>Before Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-480.webp 480w,/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-800.webp 800w,/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/preprocessed_freq/franka_state_controller_O_T_EE_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/franka_state_controller/O_T_EE @ 607Hz</p> </div> </div> <div class="row mt-4"> <div class="col-sm text-center"> <strong>After Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-480.webp 480w,/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-800.webp 800w,/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/postprocessed_freq/cartesian_impedance_controller_desired_pose_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/cartesian_impedance_controller/desired_pose @ 13Hz</p> </div> <div class="col-sm text-center"> <strong>After Sampling</strong> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-480.webp 480w,/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-800.webp 800w,/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/postprocessed_freq/franka_state_controller_O_T_EE_hist.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>/franka_state_controller/O_T_EE @ 13Hz</p> </div> </div> <div class="caption mt-2 text-center"> Frequencies of both topics are aligned after applying our sampling algorithm, from highly different original rates (50Hz vs 607Hz) to a unified 13Hz (hyperparameter). </div> <p><strong>Here is the pseudo code for our sampling algorithm which ensures equal observations from all topics:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Synchronize multiple topics to a target frequency
</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="nf">min_timestamp</span><span class="p">(</span><span class="n">topics</span><span class="p">)</span>
<span class="n">end_time</span>   <span class="o">=</span> <span class="nf">max_timestamp</span><span class="p">(</span><span class="n">topics</span><span class="p">)</span>

<span class="n">dt</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">target_freq</span>
<span class="n">t</span>  <span class="o">=</span> <span class="n">start_time</span>

<span class="k">while</span> <span class="n">t</span> <span class="o">&lt;=</span> <span class="n">end_time</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="nf">select_message</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">timestamp</span> <span class="o">&lt;=</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># the msg from the topic which has the greatest timestamp, but timestamp is &lt;= t
</span>        <span class="n">topic_buffer</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="o">=</span> <span class="n">msg</span>

    <span class="n">combined_msgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">topic_buffer</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">+=</span> <span class="n">dt</span>
</code></pre></div></div> <p>You can find the sampler package here.<a href="https://github.com/AnshPrakash/MimicPlay/tree/main/sampler"><img src="https://img.shields.io/badge/GitHub-Sampler-blue?logo=github" alt="GitHub Repo"/></a></p> <p>Further, we transform the data into robomimic style hdf5 format <a href="https://github.com/AnshPrakash/MimicPlay/tree/main/rosbag2hdf5"><img src="https://img.shields.io/badge/GitHub-rosbag2hdf5-blue?logo=github" alt="GitHub Repo"/></a></p> <blockquote> <p>The final teleoperation dataset, formatted in <strong>robomimic style</strong>, is now ready to be used in the training pipeline.</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FILE_CONTENTS {
 group      /
 group      /data
 group      /data/demo_0
 dataset    /data/demo_0/actions
 group      /data/demo_0/obs
 dataset    /data/demo_0/obs/O_T_EE
 dataset    /data/demo_0/obs/back_camera
 dataset    /data/demo_0/obs/ee_pose
 dataset    /data/demo_0/obs/front_camera
 dataset    /data/demo_0/obs/gripper_joint_states
 dataset    /data/demo_0/obs/joint_states
 dataset    /data/demo_0/obs/joint_states_desired
 group      /data/demo_1
 dataset    /data/demo_1/actions
 group      /data/demo_1/obs
 dataset    /data/demo_1/obs/O_T_EE
 dataset    /data/demo_1/obs/back_camera
 dataset    /data/demo_1/obs/ee_pose
 dataset    /data/demo_1/obs/front_camera
 dataset    /data/demo_1/obs/gripper_joint_states
 dataset    /data/demo_1/obs/joint_states
 dataset    /data/demo_1/obs/joint_states_desired
 group      /mask
 dataset    /mask/train
}
</code></pre></div></div> <hr/> <h2 id="high-level-latent-planner">High Level Latent Planner</h2> <h4 id="model">Model</h4> <p>With the collected human play data and the corresponding 3D hand trajectories ( \tau ), we formalize the latent plan learning problem as a <strong>goal-conditioned 3D trajectory generation task</strong>. In this formulation, the planner must generate feasible hand trajectories conditioned on the specified goal state.</p> <p>To model this distribution, we adopt a <strong>Gaussian Mixture Model (GMM)</strong> as the high-level planner. The GMM captures the multi-modal nature of human demonstrations, where multiple valid trajectories may exist for achieving the same goal. This provides several advantages:</p> <ul> <li><strong>Goal-conditioning</strong>: ensures that the generated trajectory is consistent with the task objective.</li> <li><strong>Flexibility</strong>: supports multiple valid solutions instead of collapsing to a single mode.</li> <li><strong>Robustness across tasks</strong>: enables the planner to generalize across diverse demonstrations collected from different tasks.</li> </ul> <p>In summary, the GMM-based planner learns to represent the distribution of goal-conditioned trajectories, which allows for generating diverse yet feasible high-level plans.</p> <h4 id="latent-plan">Latent plan</h4> <p>Our high-level planner is formulated as a <strong>latent plan generator</strong>.<br/> We use a pretrained <strong>GMM model</strong> to produce latent trajectory plans from the collected demonstrations.<br/> These latent plans are not directly executed by the robot but are instead passed to the <strong>low-level controller</strong>, which converts them into executable motor commands.<br/> This hierarchical setup defines the high-level component as a latent plan rather than direct control.</p> <h4 id="multi-modality">Multi-modality</h4> <p>The training model takes <strong>multi-modal inputs</strong> to construct the high-level planner.<br/> Specifically, it receives <strong>two-view RGB images</strong> together with the corresponding <strong>hand position information</strong> as inputs, and outputs a <strong>GMM trajectory distribution</strong>.<br/> This setup allows the model to learn from both visual context and motion data when generating latent plans.</p> <h4 id="training">Training</h4> <ul> <li> <p>Setup For the collected demonstration dataset, we used <strong>one demo as the validation set</strong>, while the remaining demos were used for <strong>training</strong>. The training was conducted following the <strong>configuration provided in the reference paper</strong>. For hyperparameters, we mainly relied on the <strong>default settings from the official repository</strong>, while performing <strong>additional tuning</strong> based on our own dataset to improve performance, e.g. “goal image range” and “std”.</p> </li> <li> <p>Evaluation We evaluated the high-level planner using two metrics:</p> </li> </ul> <ol> <li> <p><strong>GMM likelihood probability (training phase)</strong><br/> During training, we monitored the <strong>likelihood of the ground-truth data under the learned GMM model</strong>. This serves as a measure of how well the model captures the distribution of the demonstrations.</p> </li> <li> <p><strong>Distance error (test phase)</strong><br/> On the test prompts, we computed the <strong>distance error</strong> between the predicted trajectories and the ground-truth hand positions. Since our high-level planner is a <strong>probabilistic model</strong>, we performed <strong>multiple samples for each time step</strong> in the sequence. The final error metric was obtained by averaging across the entire video sequence and across all samples.</p> </li> </ol> <hr/> <h2 id="experiments">Experiments</h2> <h3 id="high-level-planner">High Level Planner</h3> <p>After completing the training of the high-level latent planner, we first collected <strong>video prompts</strong> and performed a <strong>visual inspection of the predicted trajectories</strong>. This step allowed us to qualitatively evaluate whether the generated trajectories aligned with the expected task goals and to compare them against the ground-truth trajectories from the demonstrations. Below we show example visualizations of the predicted trajectories。</p> <div class="row mt-4"> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high_level/single_view/start_with_traj-480.webp 480w,/assets/img/high_level/single_view/start_with_traj-800.webp 800w,/assets/img/high_level/single_view/start_with_traj-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/high_level/single_view/start_with_traj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>current states of hand with 10 steps future trajectory</p> </div> <div class="col-sm text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high_level/single_view/goal-480.webp 480w,/assets/img/high_level/single_view/goal-800.webp 800w,/assets/img/high_level/single_view/goal-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/high_level/single_view/goal.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>goal states of hand</p> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/high_level/single_view/traj_video.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption text-center"> trajectory through time steps </div> <hr/> <h2 id="extension-to-bimanual-tiago">Extension to Bimanual Tiago</h2> <h3 id="update-to-hand-tracking-system-to-two-hands">Update to Hand Tracking system to two hands</h3> <p>The current pretrained hand detection model is able to distinguish between the <strong>left and right hands</strong>. However, since our setup only uses <strong>two calibrated camera views</strong>, the detection results can vary significantly. One major challenge arises when the <strong>two hands occlude each other</strong>, in which case it may be impossible to reliably observe both hands in both camera views at the same time. This directly limits our ability to obtain accurate <strong>3D hand position estimates</strong> through triangulation.</p> <p>To address this issue, one potential approach we are exploring is <strong>temporal interpolation</strong>. Specifically, when a hand temporarily disappears due to occlusion, we use its <strong>2D infomation before and after the disappearance</strong> to interpolate the missing frames. By filling in these occluded intervals, we aim to maintain more consistent 3D hand trajectory estimation for bimanual tasks.</p> <hr/> <h2 id="acknowledgements">Acknowledgements</h2> <p>We would like to thank our supervisor, <a href="https://pearl-lab.com/people/franziska-herbert/">Franziska Herbert</a>, for her guidance and support throughout this project. We also extend our gratitude to the course organizer and the lab staff for providing the resources and assistance that made this work possible. Finally, we thank the authors of <a href="https://mimic-play.github.io/"><strong>MimicPlay</strong></a> for making their code publicly available.</p> <hr/> <h3 id="bibtex">BibTeX</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">prakashzhou2025mimicplay</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Prakash, Ansh and Zhou, Xiaoqi}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{MimicPlay on Franka Arm and its Extension}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://anshprakash.github.io/blog/2025/mimicplay/}}</span><span class="p">,</span>
  <span class="na">note</span>         <span class="p">=</span> <span class="s">{IROBMAN Lab Blog}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/>]]></content><author><name>Ansh Prakash</name></author><category term="Imitation-Learning,"/><category term="Learning-from-Human,"/><category term="Long-Horizon-Manipulation"/><summary type="html"><![CDATA[This blog is part of our university’s project lab, where we are working on replicating MimicPlay using a real one-arm robotic platform in our lab. Building on this setup, we aim to extend the approach to bi-manual systems such as the Tiago robot. Our work explores how abundant human play data can be leveraged to guide efficient low-level robot policies.]]></summary></entry></feed>